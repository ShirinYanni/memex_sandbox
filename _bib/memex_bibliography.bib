
@inproceedings{aeiadAdaptablePersonalisedELearning2015,
  title = {An {{Adaptable}} and {{Personalised E}}-{{Learning System Based}} on {{Free Web Resources}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Aeiad, Eiman and Meziane, Farid},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {293--299},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_26},
  abstract = {A personalised and adaptive E-Learning system architecture is developed to provide a comprehensive learning environment for learners who cannot follow a conventional programme of study. The system extract information from freely available resources on the Web, and taking into consideration the learners’ background and requirements to design modules and a planner system to facilitate the learning process. The process is supported by the development of an ontology to optimise the information extraction process. An application in the computer science field is used to evaluate the proposed system based on the IEEE/ACM Computing curriculum.},
  file = {/Users/shirin/Zotero/storage/PSKBPPGD/Aeiad and Meziane - 2015 - An Adaptable and Personalised E-Learning System Ba.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {E-Learning,Learning styles,Personalized learning},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{AlexanderStreetProQuest,
  title = {Alexander {{Street}}, a {{ProQuest Company}}},
  url = {https://video-alexanderstreet-com.uaccess.univie.ac.at/watch/python-sql-tableau-integrating-python-sql-and-tableau},
  urldate = {2021-02-01},
  abstract = {Search streaming video, audio, and text content for academic, public, and K-12 institutions. Alexander Street is an imprint of ProQuest that promotes teaching, research, and learning across music, counseling, history, anthropology, drama, film, and more.},
  file = {/Users/shirin/Zotero/storage/JBZILXA5/python-sql-tableau-integrating-python-sql-and-tableau.html},
  langid = {english}
}

@article{allenColeNussbaumerKnaflic2019,
  title = {Cole {{Nussbaumer Knaflic}}. {{Storytelling}} with Data: {{A}} Data Visualization Guide for Business Professionals. {{Hoboken}}, {{NJ}}: {{Wiley}}, 2015, 288 Pages, \$39.95 Paperback},
  shorttitle = {Cole {{Nussbaumer Knaflic}}. {{Storytelling}} with Data},
  author = {Allen, Danielle B.},
  date = {2019},
  journaltitle = {Personnel Psychology},
  volume = {72},
  pages = {331--333},
  issn = {1744-6570},
  doi = {10.1111/peps.12333},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/peps.12333},
  urldate = {2021-02-01},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/peps.12333},
  file = {/Users/shirin/Zotero/storage/B5P7DUHU/Allen - 2019 - Cole Nussbaumer Knaflic. Storytelling with data A.pdf;/Users/shirin/Zotero/storage/7VKMQWK9/peps.html},
  langid = {english},
  number = {2}
}

@online{AnnotatingCollaborativelyCollaborative,
  title = {Annotating {{Collaboratively}} - {{Collaborative Annotation}} for {{Reliable Natural Language Processing}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary-wiley-com.uaccess.univie.ac.at/doi/10.1002/9781119306696.ch1},
  urldate = {2020-10-09},
  file = {/Users/shirin/Zotero/storage/4SD723BX/9781119306696.html}
}

@book{ApplyingColorTheory2016,
  title = {Applying {{Color Theory}} to {{Digital Media}} and {{Visualization}}},
  date = {2016-10-14},
  publisher = {{CRC Press}},
  doi = {10.1201/9781315380384},
  url = {https://www-taylorfrancis-com.uaccess.univie.ac.at/books/applying-color-theory-digital-media-visualization-theresa-marie-rhyne/10.1201/9781315380384},
  urldate = {2021-02-02},
  abstract = {This book provides an overview of the application of color theory concepts to digital media and visualization. It highlights specific color concepts like},
  file = {/Users/shirin/Zotero/storage/ITD3T53S/2016 - Applying Color Theory to Digital Media and Visuali.pdf},
  isbn = {978-1-315-38038-4},
  langid = {english}
}

@inproceedings{arnulphySupervisedMachineLearning2015,
  title = {Supervised {{Machine Learning Techniques}} to {{Detect TimeML Events}} in {{French}} and {{English}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Arnulphy, Béatrice and Claveau, Vincent and Tannier, Xavier and Vilnat, Anne},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {19--32},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_2},
  abstract = {Identifying events from texts is an information extraction task necessary for many NLP applications. Through the TimeML specifications and TempEval challenges, it has received some attention in recent years. However, no reference result is available for French. In this paper, we try to fill this gap by proposing several event extraction systems, combining for instance Conditional Random Fields, language modeling and k-nearest-neighbors. These systems are evaluated on French corpora and compared with state-of-the-art methods on English. The very good results obtained on both languages validate our approach.},
  file = {/Users/shirin/Zotero/storage/H9TWMTNE/Arnulphy et al. - 2015 - Supervised Machine Learning Techniques to Detect T.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {CRF,English,Event identification,French,Information extraction,Language modeling,TempEval,TimeML},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{bergNADIASimplifiedApproach2015,
  title = {{{NADIA}}: {{A Simplified Approach Towards}} the {{Development}} of {{Natural Dialogue Systems}}},
  shorttitle = {{{NADIA}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Berg, Markus M.},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {144--150},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_12},
  abstract = {Spoken Dialogue Systems have enormously improved during the last couple of years and gave rise to voice-controlled mobile assistants. While the abilities of these systems are very sophisticated, there is a lack of tools enabling us to easily describe a natural dialogue that can afterwards be processed by a dialogue engine without having to programme the engine itself. In this paper we present NADIA, a dialogue engine that can process an easy to define XML-based dialogue description.},
  file = {/Users/shirin/Zotero/storage/29YND5ES/Berg - 2015 - NADIA A Simplified Approach Towards the Developme.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Dialogue Modelling,Natural Language Generation,Spoken Dialogue Systems,Voice User Interface},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@thesis{bergnerIntegratingNaturalLanguage2015,
  title = {Integrating Natural Language Processing with Semantic-Based Modeling},
  author = {Bergner, Martin},
  date = {2015},
  institution = {{uniwien}},
  location = {{wien}},
  url = {http://othes.univie.ac.at/38679/},
  urldate = {2021-02-02},
  abstract = {Die Evolution des world wide webs bereitete den Weg für die Verbreitung von Social Media Plattformen über die ganze Welt. Dabei veränderte sich nicht nur die Art und Weise wie Menschen miteinander kommunizieren, sondern auch wie Unternehmen mit potenziellen Kunden interagieren. Globalisierung und dieser neue Weg der Interaktion übt enormen Druck auf Unternehmen aus, wettbewerbsfähig zu bleiben. Eine Strategie ist der Fokus auf Geschäftsprozessoptimierungs-Initiativen um, durchgehend Kosten zu senken und Effizienz zu steigern unter Nutzung von Mitarbeiter- und Kundenwissen. Während Social Media Plattformen für Unternehmen hinsichtlich des Marketings mittlerweile weit verbreitet sind, wird der enorme Umfang an Daten, generiert von den Usern und so enorm wichtig für diese Geschäftsprozessoptimierungs-Initiativen, kaum verarbeitet und in nutzbares Wissen verwandelt. Diese Arbeit schlägt eine Methode vor, welche die Daten aus Social Media Plattformen unter Verwendung von natürlicher Sprachverarbeitung und semantischen Modellierungsmethoden als Erweiterung der SeMFIS Modellierungsmethoden basierend auf der ADOxx Metamodeling Platform verarbeitet. Dies schließt die technische Implementierung in Java, als auch eine nachfolgende Evaluierung der gewonnenen Ergebnisse hinsichtlich Objektivität und Validität unter Verwendung von Daten aus dem Facebook Auftritts einer Airline und gewonnenen Daten, basierend auf dem Wissen von menschlichen Akteuren, ein. --- mit CD},
  file = {/Users/shirin/Zotero/storage/DKSIHSHW/Bergner - 2015 - Integrating natural language processing with seman.pdf;/Users/shirin/Zotero/storage/SEM3GRTG/38679.html},
  pagetotal = {xvi, 167 Seiten : Illustrationen, Diagramme},
  type = {mathesis}
}

@online{BuildingResponsiveData,
  title = {Building responsive data visualization for the Web - Universit\&\#228;t Wien},
  url = {https://usearch.univie.ac.at/primo-explore/fulldisplay/UWI_alma51380208650003332/UWI},
  urldate = {2021-02-01},
  abstract = {Building responsive data visualization for the Web},
  file = {/Users/shirin/Zotero/storage/FZS3AY93/fulldisplay.html},
  langid = {german}
}

@inproceedings{castellucciAcquiringLargeScale2015,
  title = {Acquiring a {{Large Scale Polarity Lexicon Through Unsupervised Distributional Methods}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Castellucci, Giuseppe and Croce, Danilo and Basili, Roberto},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {73--86},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_6},
  abstract = {The recent interests in Sentiment Analysis systems brought the attention on the definition of effective methods to detect opinions and sentiments in texts with a good accuracy. Many approaches that can be found in literature are based on hand-coded resources that model the prior polarity of words or multi-word expressions. The construction of such resources is in general expensive and coverage issues arise with respect to the multiplicity of linguistic phenomena of sentiment expressions. This paper presents an automatic method for deriving a large-scale polarity lexicon based on Distributional Models of lexical semantics. Given a set of sentences annotated with polarity, we transfer the sentiment information from sentences to words. The set of annotated examples is derived from Twitter and the polarity assignment to sentences is derived by simple heuristics. The approach is mostly unsupervised, and the experimental evaluation carried out on two Sentiment Analysis tasks shows the benefits of the generated resource.},
  file = {/Users/shirin/Zotero/storage/PSSMSC8Q/Castellucci et al. - 2015 - Acquiring a Large Scale Polarity Lexicon Through U.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Distributional semantics,Polarity lexicon generation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{colomboControlledNaturalLanguage2015,
  title = {A {{Controlled Natural Language}} for {{Business Intelligence Monitoring}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Colombo, Christian and Grech, Jean-Paul and Pace, Gordon J.},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {300--306},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_27},
  abstract = {With ever increasing information available in social networks, the number of businesses attempting to exploit it is on the rise, particularly by keeping track of their customers’ posts and likes on social media sites like Facebook. Whilst APIs can be used to automate the tracking process, writing scripts to extract information and process it requires considerable technical skill and is thus not an option for non technical business analysts. On the other hand, off-the-shelf business intelligence solutions do not provide the desired flexibility for the specific needs of particular businesses. In this paper, we present a controlled natural language enabling non-technical users to express their queries in a language they can easily understand but which can be directly compiled into executable code.},
  file = {/Users/shirin/Zotero/storage/DMV4R9YN/Colombo et al. - 2015 - A Controlled Natural Language for Business Intelli.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Controlled natural languages,Runtime verification,Social networks},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{ComputerScience2008,
  title = {Computer Science.},
  date = {2008},
  publisher = {{Springer,}},
  location = {{Berlin :}},
  issn = {1865-2042},
  abstract = {Refereed/Peer-reviewed, "Research and development.", SpringerLink website lists former and later titles., First issue; title from journal home page (SpringerLink, viewed Mar. 20, 2008)., Vol. 33, issue 3/4 (Aug. 2018) (SpringerLink, viewed Dec. 20, 2019).},
  keywords = {Computer science,Computer science ; Periodicals,Periodicals$$QPeriodicals},
  langid = {english}
}

@inproceedings{cossuAutomaticClassificationPLSPM2015,
  title = {Automatic {{Classification}} and {{PLS}}-{{PM Modeling}} for {{Profiling Reputation}} of {{Corporate Entities}} on {{Twitter}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Cossu, Jean-Valère and Sanjuan, Eric and Torres-Moreno, Juan-Manuel and El-Bèze, Marc},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {282--289},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_25},
  abstract = {In this paper, we address the task of detecting the reputation alert in social media updates, that is, deciding whether a new-coming content has strong and immediate implications for the reputation of a given entity. This content is also submitted to a standard typology of reputation dimensions that consists in a broad classification of the aspects of an under public audience company. Reputation manager needs a real-time database and method to report what is happening right now to his brand. However, typical Natural Language Processing (NLP) approaches to these tasks require external resources and show non-relational modeling. We propose a fast supervised approach for extracting textual features, which we use to train simple statistical reputation classifiers. These classifiers outputs are used in a Partial Least Squares Path Modeling (PLS-PM) system to model the reputation. Experiments on the RepLab 2013 and 2014 collections show that our approaches perform as well as the state-of-the-art more complex methods.},
  file = {/Users/shirin/Zotero/storage/TPMUIGMC/Cossu et al. - 2015 - Automatic Classification and PLS-PM Modeling for P.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Real-time Database,RepLab,Reputation Management,Tweets,Variable Alert},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{CreatingDataStories,
  title = {Creating {{Data Stories}} with {{Tableau Public}}: {{EBSCOhost}}},
  url = {http://web.b.ebscohost.com.uaccess.univie.ac.at/ehost/detail/detail?vid=0&sid=1c3abe23-050f-4d04-aca9-edb171ec07d9%40pdc-v-sessmgr05&bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3d#AN=1104627&db=nlebk},
  urldate = {2021-02-01},
  file = {/Users/shirin/Zotero/storage/L9V7YMQS/detail.html}
}

@inproceedings{czerepowickaSEJFGrammaticalLexicon2018,
  title = {{{SEJF}} - {{A Grammatical Lexicon}} of {{Polish Multiword Expressions}}},
  booktitle = {Human {{Language Technology}}. {{Challenges}} for {{Computer Science}} and {{Linguistics}}},
  author = {Czerepowicka, Monika and Savary, Agata},
  editor = {Vetulani, Zygmunt and Mariani, Joseph and Kubis, Marek},
  date = {2018},
  pages = {59--73},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-93782-3_5},
  abstract = {We present SEJF, a lexical resource of Polish nominal, adjectival and adverbial multiword expressions. It consists of an intensional module with about 4,700 multiword lemmas assigned to 160 inflection graphs, and an extensional module with 88,000 automatically generated inflected forms annotated with grammatical tags. We show the results of its coverage evaluation against an annotated corpus. The resource is freely available under the Creative Commons BY-SA license.},
  file = {/Users/shirin/Zotero/storage/U8HJS4DU/Czerepowicka und Savary - 2018 - SEJF - A Grammatical Lexicon of Polish Multiword E.pdf},
  isbn = {978-3-319-93782-3},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{DataVisualizationCharts2018,
  title = {Data {{Visualization}} : {{Charts}}, {{Maps}}, and {{Interactive Graphics}}},
  shorttitle = {Data {{Visualization}}},
  date = {2018-12-07},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315201351},
  url = {https://www-taylorfrancis-com.uaccess.univie.ac.at/books/data-visualization-robert-grant/10.1201/9781315201351},
  urldate = {2021-02-01},
  abstract = {This is the age of data. There are more innovations and more opportunities for interesting work with data than ever before, but there is also an overwhelming},
  file = {/Users/shirin/Zotero/storage/TLLRMXKW/2018 - Data Visualization  Charts, Maps, and Interactive.pdf;/Users/shirin/Zotero/storage/2A4RV6LE/9781315201351.html},
  isbn = {978-1-315-20135-1},
  langid = {english}
}

@book{davisUniversalComputerRoad2018,
  title = {The Universal Computer: : The Road from {{Leibniz}} to {{Turing}}},
  shorttitle = {The Universal Computer},
  author = {Davis, Martin},
  date = {2018},
  edition = {Third edition..},
  publisher = {{CRC Press,}},
  location = {{Boca Raton, Florida :}},
  url = {https://www.taylorfrancis.com/books/9781315144726},
  urldate = {2020-10-06},
  isbn = {978-1-315-14472-6},
  keywords = {Electronic digital computers,Mathematicians ; Biography},
  langid = {english}
}

@online{DblpAppliedNatural,
  title = {Dblp: {{Applied Natural Language Processing Conference}}},
  url = {https://dblp.uni-trier.de/db/conf/anlp/index.html},
  urldate = {2020-10-09},
  file = {/Users/shirin/Zotero/storage/VTQRDQ2N/index.html}
}

@inproceedings{delmonteAutomaticDetectionModality2015,
  title = {Automatic {{Detection}} of {{Modality}} with {{ITGETARUNS}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Delmonte, Rodolfo},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {404--411},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_38},
  abstract = {In this paper we present a system for modality detection which is then used for Subjectivity and Factuality evaluation. The system has been tested lately on a task for Subjectivity and Irony detection in Italian tweets (http://www.di.unito.it/\textasciitilde tutreeb/sentipolc-evalita14/index.html), where the performance was 10th and 4th, respectively, over 27 participants overall. We will focus our paper on an internal evaluation where we considered three national newspapers Il Corriere, Repubblica, Libero. This task was prompted by a project on the evaluation of press stylistic features in political discourse. The project used newspaper articles from the same sources over a period of three months, thus including latest political 2013 governmental crisis. We intended to produce a similar experiment and evaluate results in comparison with previous 2011 crisis. In this evaluation, we focused on Subjectivity, Polarity and Factuality which include Modality evaluation. Final graphs at the end of the paper will show results confirming our previous findings about differences in style, with Il Corriere emerging as the most atypical.},
  file = {/Users/shirin/Zotero/storage/UCG5Z56N/Delmonte - 2015 - Automatic Detection of Modality with ITGETARUNS.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Dependency Parser,Intensional Verb,Modality Annotation,Relative Clause,Word Sense Disambiguation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{duImprovingSupervisedClassification2015,
  title = {Improving {{Supervised Classification Using Information Extraction}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Du, Mian and Pierce, Matthew and Pivovarova, Lidia and Yangarber, Roman},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {3--18},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_1},
  abstract = {We explore supervised learning for multi-class, multi-label text classification, focusing on real-world settings, where the distribution of labels changes dynamically over time. We use the PULS Information Extraction system to collect information about the distribution of class labels over named entities found in text. We then combine a knowledge-based rote classifier with statistical classifiers to obtain better performance than either classification method alone. The resulting classifier yields a significant improvement in macro-averaged F-measure compared to the state of the art, while maintaining comparable micro-average.},
  file = {/Users/shirin/Zotero/storage/L7VTXX49/Du et al. - 2015 - Improving Supervised Classification Using Informat.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Feature Selection Method,Industry Sector,Information Extraction,Name Entity Recognition,Statistical Classifier},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{ekpenyongIntelligentSpeechFeatures2018,
  title = {Intelligent {{Speech Features Mining}} for {{Robust Synthesis System Evaluation}}},
  booktitle = {Human {{Language Technology}}. {{Challenges}} for {{Computer Science}} and {{Linguistics}}},
  author = {Ekpenyong, Moses E. and Inyang, Udoinyang G. and Ekong, Victor E.},
  editor = {Vetulani, Zygmunt and Mariani, Joseph and Kubis, Marek},
  date = {2018},
  pages = {3--18},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-93782-3_1},
  abstract = {Speech synthesis evaluation involves the analytical description of useful features, sufficient to assess the performance of a speech synthesis system. Its primary focus is to determine the degree of semblance of synthetic voice to a natural or human voice. The task of evaluation is usually driven by two methods: the subjective and objective methods, which have indeed become a regular standard for evaluating voice quality, but are mostly challenged by high speech variability as well as human discernment errors. Machine learning (ML) techniques have proven to be successful in the determination and enhancement of speech quality. Hence, this contribution utilizes both supervised and unsupervised ML tools to recognize and classify speech quality classes. Data were collected from a listening test (experiment) and the speech quality assessed by domain experts for naturalness, intelligibility, comprehensibility, as well as, tone, vowel and consonant correctness. During the pre-processing stage, a Principal Component Analysis (PCA) identified 4 principal components (intelligibility, naturalness, comprehensibility and tone) – accounting for 76.79\% variability in the dataset. An unsupervised visualization using self organizing map (SOM), then discovered five distinct target clusters with high densities of instances, and showed modest correlation between significant input factors. A Pattern recognition using deep neural network (DNN), produced a confusion matrix with an overall performance accuracy of 93.1\%, thus signifying an excellent classification system.},
  file = {/Users/shirin/Zotero/storage/B8BLPM9H/Ekpenyong et al. - 2018 - Intelligent Speech Features Mining for Robust Synt.pdf},
  isbn = {978-3-319-93782-3},
  keywords = {Deep neural network,Dimension reduction,Machine learning,Pattern recognition,Speech quality evaluation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{entrupGermanLanguageProcessing2015,
  title = {({{German}}) {{Language Processing}} for {{Lucene}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Entrup, Bastian},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {390--394},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_35},
  abstract = {This paper introduces an open-source Java-package called German Language Processing for Lucene (glp4lucene). Although it was originally developed to work with German texts, it is to a large degree language independent. It aims at facilitating four language processing steps for working with non-English texts and Apache Lucene/Solr: lemmatizing words, weighting terms based on their part-of-speech, adding synonyms and decompounding nouns, without the necessity of a thorough understanding of natural language processing.},
  file = {/Users/shirin/Zotero/storage/5JK3UXUQ/Entrup - 2015 - (German) Language Processing for Lucene.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Digital Humanities Projects,Distributional Thesaurus,German Text,Language Processing Steps,Synonym Expansion},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{FullArticleDigital,
  title = {Full Article: {{Digital}} Education Governance: Data Visualization, Predictive Analytics, and ‘Real-Time’ Policy Instruments},
  url = {https://www-tandfonline-com.uaccess.univie.ac.at/doi/full/10.1080/02680939.2015.1035758},
  urldate = {2021-02-01},
  file = {/Users/shirin/Zotero/storage/U9IDQIT2/02680939.2015.html}
}

@book{ghavamiBigDataAnalytics2019,
  title = {Big {{Data Analytics Methods}}: : {{Analytics Techniques}} in {{Data Mining}}, {{Deep Learning}} and {{Natural Language Processing}}},
  shorttitle = {Big {{Data Analytics Methods}}},
  author = {Ghavami, Peter},
  date = {2019},
  edition = {2nd Edition.},
  publisher = {{De Gruyter,}},
  location = {{Berlin ; Boston :}},
  url = {https://doi.org/10.1515/9781547401567},
  urldate = {2020-10-13},
  abstract = {Big Data Analytics Methods unveils secrets to advanced analytics techniques ranging from machine learning, random forest classifiers, predictive modeling, cluster analysis, natural language processing (NLP), Kalman filtering and ensembles of models for optimal accuracy of analysis and prediction. More than 100 analytics techniques and methods provide big data professionals, business intelligence professionals and citizen data scientists insight on how to overcome challenges and avoid common pitfalls and traps in data analytics. The book offers solutions and tips on handling missing data, noisy and dirty data, error reduction and boosting signal to reduce noise. It discusses data visualization, prediction, optimization, artificial intelligence, regression analysis, the Cox hazard model and many analytics using case examples with applications in the healthcare, transportation, retail, telecommunication, consulting, manufacturing, energy and financial services industries. This book's state of the art treatment of advanced data analytics methods and important best practices will help readers succeed in data analytics.},
  isbn = {978-1-5474-0156-7},
  keywords = {Big data,BUSINESS & ECONOMICS,Data analysis,Data mining,Information Management,Machine learning,Neural networks},
  langid = {english},
  pagetotal = {xvi+238}
}

@article{gualaModelsMediatorsPerspectives2001,
  title = {Models as {{Mediators}}. {{Perspectives}} on {{Natural}} and {{Social Science}}, {{Mary S}}. {{Morgan}} and {{Margaret Morrison}} (Eds.). {{Cambridge University Press}}, 1999, Xi + 401 Pages},
  author = {Guala, Francesco and Psillos, Stathis},
  date = {2001},
  journaltitle = {Economics and philosophy},
  volume = {17},
  pages = {275--294},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, UK}},
  issn = {0266-2671},
  doi = {10.1017/S0266267101230272},
  langid = {english},
  number = {2}
}

@inproceedings{habibiQueryRefinementUsing2015,
  title = {Query {{Refinement Using Conversational Context}}: {{A Method}} and an {{Evaluation Resource}}},
  shorttitle = {Query {{Refinement Using Conversational Context}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Habibi, Maryam and Popescu-Belis, Andrei},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {89--102},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_7},
  abstract = {This paper introduces a query refinement method applied to queries asked by users during a meeting or a conversation. The proposed method does not require further clarifications from users, to avoid distracting them from their conversation, but leverages instead the local context of the conversation. The method first represents the local context by extracting keywords from the transcript of the conversation. It then expands the queries with keywords that best represent the topic of the query, i.e. expansion keywords accompanied by weights indicating their topical similarity to the query. Moreover, we present a dataset called AREX and an evaluation metric based on relevance judgments collected in a crowdsourcing experiment. We compare our query expansion approach with other methods, over queries extracted from the AREX dataset, showing the superiority of our method when either manual or automatic transcripts of the AMI Meeting Corpus are used.},
  file = {/Users/shirin/Zotero/storage/2USJ5LZC/Habibi and Popescu-Belis - 2015 - Query Refinement Using Conversational Context A M.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Crowdsourcing,Evaluation,Query refinement,Speech-based information retrieval},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{hagenWhatWasQuery2015,
  title = {What Was the {{Query}}? {{Generating Queries}} for {{Document Sets}} with {{Applications}} in {{Cluster Labeling}}},
  shorttitle = {What Was the {{Query}}?},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Hagen, Matthias and Michel, Maximilian and Stein, Benno},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {124--133},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_10},
  abstract = {We deal with the task of generating a query that retrieves a given set of documents. In its abstract form, this can be seen as a “compression” of the document set to a short query. But the task also has a real-world application: cluster labeling (e.g., for faceted search). Our solution to cluster labeling is the usage of queries that approximately retrieve a cluster’s documents. To be generalizable, our approach does not require access to a search index but only a public interface like an API. This way, our approach can also be implemented at client side.In an experimental evaluation, a basic version of our approach using a simple retrieval model is on par with standard cluster labeling techniques. A further user study reveals that queries as labels are often preferred when they are not too long.},
  file = {/Users/shirin/Zotero/storage/WT7DQBQ8/Hagen et al. - 2015 - What was the Query Generating Queries for Documen.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Cluster Label,Jaccard Index,Retrieval Model,Search Engine,User Study},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{hakimovApplyingSemanticParsing2015,
  title = {Applying {{Semantic Parsing}} to {{Question Answering Over Linked Data}}: {{Addressing}} the {{Lexical Gap}}},
  shorttitle = {Applying {{Semantic Parsing}} to {{Question Answering Over Linked Data}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Hakimov, Sherzod and Unger, Christina and Walter, Sebastian and Cimiano, Philipp},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {103--109},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_8},
  abstract = {Question answering over linked data has emerged in the past years as an important topic of research in order to provide natural language access to a growing body of linked open data on the Web. In this paper we focus on analyzing the lexical gap that arises as a challenge for any such question answering system. The lexical gap refers to the mismatch between the vocabulary used in a user question and the vocabulary used in the relevant dataset. We implement a semantic parsing approach and evaluate it on the QALD-4 benchmark, showing that the performance of such an approach suffers from training data sparseness. Its performance can, however, be substantially improved if the right lexical knowledge is available. To show this, we model a set of lexical entries by hand to quantify the number of entries that would be needed. Further, we analyze if a state-of-the-art tool for inducing ontology lexica from corpora can derive these lexical entries automatically. We conclude that further research and investments are needed to derive such lexical knowledge automatically or semi-automatically.},
  file = {/Users/shirin/Zotero/storage/ERXYVB6U/Hakimov et al. - 2015 - Applying Semantic Parsing to Question Answering Ov.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Lexical Entry,Lexical Item,Parse Tree,Question Answering,Semantic Representation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{HBRMcKinseyAward,
  title = {{{HBR McKinsey Award}} Winners: {{Good}} Management Matters More than We Realize | {{McKinsey}} \& {{Company}}},
  shorttitle = {{{HBR McKinsey Award}} Winners},
  url = {https://www.mckinsey.com/about-us/new-at-mckinsey-blog/hbr-mckinsey-award-winners-good-management-matters-more-than-we-realize},
  urldate = {2020-10-08},
  abstract = {This spring, we announced the winner of the HBR McKinsey Award for the best Harvard Business Review article of 2017: \&ldquo;Why do we undervalue competent management?,\&rdquo; by professors Raffaella Sadun, Nicholas Bloom, and John Van Reenen.},
  file = {/Users/shirin/Zotero/storage/7NZMDEJH/hbr-mckinsey-award-winners-good-management-matters-more-than-we-realize.html},
  langid = {english}
}

@inproceedings{hoenenSimulatingMisreading2015,
  title = {Simulating {{Misreading}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Hoenen, Armin},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {385--389},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_34},
  abstract = {Physical misreading (as opposed to interpretational misreading) is an unnoticed substitution in silent reading. Especially for legally important documents or instruction manuals, this can lead to serious consequences. We present a prototype of an automatic highlighter targeting words which can most easily be misread in a given text using a dynamic orthographic neighbour concept. We propose measures of fit of a misread token based on Natural Language Processing and detect a list of short most easily misread tokens in the English language. We design a highlighting scheme for avoidance of misreading.},
  file = {/Users/shirin/Zotero/storage/EXDNYN3X/Hoenen - 2015 - Simulating Misreading.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Lectio difficilior,Misreading,Reading simulation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{illinaNeuralNetworksRevisited2018,
  title = {Neural {{Networks Revisited}} for {{Proper Name Retrieval}} from {{Diachronic Documents}}},
  booktitle = {Human {{Language Technology}}. {{Challenges}} for {{Computer Science}} and {{Linguistics}}},
  author = {Illina, Irina and Fohr, Dominique},
  editor = {Vetulani, Zygmunt and Mariani, Joseph and Kubis, Marek},
  date = {2018},
  pages = {19--30},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-93782-3_2},
  abstract = {Developing high-quality transcription systems for very large vocabulary corpora is a challenging task. Proper names are usually key to understanding the information contained in a document. To increase the vocabulary coverage, a huge amount of text data should be used. In this paper, we extend the previously proposed neural networks for word embedding models: word vector representation proposed by Mikolov is enriched by an additional non-linear transformation. This model allows to better take into account lexical and semantic word relationships. In the context of broadcast news transcription and in terms of recall, experimental results show a good ability of the proposed model to select new relevant proper names.},
  file = {/Users/shirin/Zotero/storage/4SQF8N34/Illina und Fohr - 2018 - Neural Networks Revisited for Proper Name Retrieva.pdf},
  isbn = {978-3-319-93782-3},
  keywords = {Neural networks,Out-of-vocabulary words,Proper names,Speech recognition,Vocabulary extension},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{InformationWissenschaftPraxis,
  title = {Information - {{Wissenschaft}} \& {{Praxis}}},
  url = {https://www-degruyter-com.uaccess.univie.ac.at/journal/key/IWP/html},
  urldate = {2021-02-02},
  file = {/Users/shirin/Zotero/storage/S7I7ILFL/html.html}
}

@inproceedings{janzenBenevolentSalesAssistants2015,
  title = {Towards {{Benevolent Sales Assistants}} in {{Retailing Scenarios}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Janzen, Sabine and Maass, Wolfgang},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {180--193},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_16},
  abstract = {Non-collaborative dialogues like sales dialogues are characterized by congruent intentions, i.e. intentions that are agreed by dialogue partners, and conflicting intentions. We will refer to these intentional structures as mixed intention sets. In this paper, we will investigate dialogue systems that are benevolent towards a dialogue partner, i.e. benevolent agents try to find a fair balance between partner intentions and agent intentions in particular with respect to conflicting intentions. For the class of question-answering dialogues, we propose a model for the intelligent generation of answers considering mixed intention sets and demonstrate its application in the retailing domain in form of a benevolent sales assistant (BSA). BSA processes mixed intention sets in a strategic way by means of a game-theoretical equilibrium approach to find a fair balance between intentions of dialogue partners. We evaluated the BSA by a run-time analysis of 500 simulated sales dialogues between customers and retailers and show how the sales assistant strategically generates answers considering mixed intention sets in retailing scenarios.},
  file = {/Users/shirin/Zotero/storage/VQAQTP22/Janzen and Maass - 2015 - Towards Benevolent Sales Assistants in Retailing S.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Dialogue Partner,Dialogue System,Fair Balance,Nash Equilibrium,Text Plan},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{jurgovskyComparingRecursiveAutoencoder2015,
  title = {Comparing {{Recursive Autoencoder}} and {{Convolutional Network}} for {{Phrase}}-{{Level Sentiment Polarity Classification}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Jurgovsky, Johannes and Granitzer, Michael},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {160--166},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_14},
  abstract = {We present a comparative evaluation of two neural network architectures, which can be used to compute representations of phrases or sentences. The Semi-Supervised Recursive Autoencoder (SRAE) and the Convolutional Neural Network (CNN) are both methods that directly operate on sequences of words represented via word embeddings and jointly model the syntactic and semantic peculiarities of phrases. We compare both models with respect to their classification accuracy on the task of binary sentiment polarity classification. Our evaluation shows that a single-layer CNN produces equally accurate phrase representations and that both methods profit from the initialization with word embeddings trained by a language model. We observe that the initialization with domain specific word embeddings has no significant effect on the accuracy of both phrase models. A pruning experiment revealed that up to 95 \% of the parameters used to train the CNN could be removed afterwards without affecting the model’s accuracy.},
  file = {/Users/shirin/Zotero/storage/L8LDDW7V/Jurgovsky and Granitzer - 2015 - Comparing Recursive Autoencoder and Convolutional .pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Artificial neural networks,Convolutional neural network,Deep learning,Natural language processing,Recursive autoencoder},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{kastratiUsingContextAwareSemantic2015,
  title = {Using {{Context}}-{{Aware}} and {{Semantic Similarity Based Model}} to {{Enrich Ontology Concepts}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Kastrati, Zenun and Yayilgan, Sule Yildirim and Imran, Ali Shariq},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {137--143},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_11},
  abstract = {Domain ontologies are a good starting point to model in a formal way the basic vocabulary of a given domain. However, in order for an ontology to be usable in real applications, it has to be supplemented with lexical resources of this particular domain. The learning process of enriching domain ontologies with new lexical resources employed in the existing approaches takes into account only the contextual aspects of terms and does not consider their semantics. Therefore, this paper proposes a new objective metric namely SEMCON which combines contextual as well as semantic information of terms to enriching the domain ontology with new concepts. The SEMCON defines the context by first computing an observation matrix which exploits the statistical features such as frequency of the occurrence of a term, term’s font type and font size. The semantics is then incorporated by computing a semantic similarity score using lexical database WordNet. Subjective and objective experiments are conducted and results show an improved performance of SEMCON compared with tf*idf and \textbackslash (\textbackslash chi \^\{2\}\textbackslash ).},
  file = {/Users/shirin/Zotero/storage/24P5255R/Kastrati et al. - 2015 - Using Context-Aware and Semantic Similarity Based .pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Concept,Context aware,Domain ontology,Semantic similarity},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{krollAssociatingIntentSentiment2015,
  title = {Associating {{Intent}} with {{Sentiment}} in {{Weblogs}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Kröll, Mark and Strohmaier, Markus},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {212--219},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_19},
  abstract = {People willingly provide more and more information about themselves on social media platforms. This personal information about users’ emotions (sentiment) or goals (intent) is particularly valuable, for instance, for monitoring tools. So far, sentiment and intent analysis were conducted separately. Yet, both aspects can complement each other thereby informing processes such as explanation and reasoning. In this paper, we investigate the relation between intent and sentiment in weblogs. We therefore extract \textasciitilde 90,000 human goal instances from the ICWSM 2009 Spinn3r dataset and assign respective sentiments. Our results indicate that associating intent with sentiment represents a valuable addition to research areas such as text analytics and text understanding.},
  file = {/Users/shirin/Zotero/storage/MM585NVP/Kröll and Strohmaier - 2015 - Associating Intent with Sentiment in Weblogs.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Human goals,Intent analysis,Sentiment,Weblogs},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{kucukHighPrecisionPersonName2015,
  title = {High-{{Precision Person Name Extraction}} from {{Turkish Texts Using Wikipedia}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Küçük, Dilek and Küçük, Doğan},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {347--354},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_31},
  abstract = {In this paper, we focus on person name extraction from diverse text types in Turkish and have compiled a large set of person names from Turkish Wikipedia. After automated post-processing to clean and extend it, we have performed extraction experiments using this resource on data sets of considerable sizes and achieved high precision rates. Next, we have shown that the use of non-local dependencies together with this Wikipedia resource improves recall, and hence F-Measure, considerably. Finally, we have tested the contribution of the resource and the scheme based on non-local dependencies to the person name extraction performance of a full-fledged named entity recognizer.},
  file = {/Users/shirin/Zotero/storage/SDW48IZQ/Küçük and Küçük - 2015 - High-Precision Person Name Extraction from Turkish.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Named entity,Person name extraction,Turkish,Wikipedia},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{kumarguptaPSOASentFeatureSelection2015,
  title = {{{PSO}}-{{ASent}}: {{Feature Selection Using Particle Swarm Optimization}} for {{Aspect Based Sentiment Analysis}}},
  shorttitle = {{{PSO}}-{{ASent}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Kumar Gupta, Deepak and Srikanth Reddy, Kandula and {Shweta} and Ekbal, Asif},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {220--233},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_20},
  abstract = {The amount of user generated online contents has increased dramatically in the recent past. The phenomenal growth of e-commerce has led to a significantly large number of reviews for a product or service. This provides useful information to the users to take a fully informed decision on whether to acquire the service and/or product or not. In this paper we present a method for automatic feature selection for aspect term extraction and sentiment classification. The proposed approach is based on the principle of Particle Swarm Optimization (PSO) and performs feature selection within the learning framework of Conditional Random Field (CRF). Experiments on the benchmark set up of SemEval-2014 Aspect based Sentiment Analysis Shared Task show the F-measure values of 81.91 \% and 72.42 \% for aspect term extraction in the laptop and restaurant domains, respectively. The method yields the classification accuracies of 78.48 \% for the restaurant and 71.25 \% for the laptop domain. Comparisons with the baselines and other existing systems show that our proposed approach attains the promising accuracies with much reduced feature sets in all the settings.},
  file = {/Users/shirin/Zotero/storage/E9B5KBHL/Kumar Gupta et al. - 2015 - PSO-ASent Feature Selection Using Particle Swarm .pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Aspect extraction,Conditional random field,Feature selection,Particle Swarm Optimization,Sentiment analysis},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{lawlessTextSummarizationSpeech2015,
  title = {Text {{Summarization}} and {{Speech Synthesis}} for the {{Automated Generation}} of {{Personalized Audio Presentations}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Lawless, Séamus and Lavin, Peter and Bayomi, Mostafa and Cabral, João P. and Ghorab, M. Rami},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {307--320},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_28},
  abstract = {In today’s fast-paced world, users face the challenge of having to consume a lot of content in a short time. This situation is exacerbated by the fact that content is scattered in a range of different languages and locations. This research addresses these challenges using a number of natural language processing techniques: adapting content using automatic text summarization; enhancing content accessibility through machine translation; and altering the delivery modality through speech synthesis. This paper introduces Lean-back Learning (LbL), an information system that delivers automatically generated audio presentations for consumption in a “lean-back” fashion, i.e. hands-busy, eyes-busy situations. These presentations are personalized and are generated using multilingual multi-document text summarization. The paper discusses the system’s components and algorithms, in addition to initial system evaluations.},
  file = {/Users/shirin/Zotero/storage/E3FYZBPV/Lawless et al. - 2015 - Text Summarization and Speech Synthesis for the Au.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Lean-back learning,Multilingual content adaptation,personalization,Speech synthesis,Text summarization},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{levDefenseWordEmbedding2015,
  title = {In {{Defense}} of {{Word Embedding}} for {{Generic Text Representation}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Lev, Guy and Klein, Benjamin and Wolf, Lior},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {35--50},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_3},
  abstract = {Statistical methods have shown a remarkable ability to capture semantics. The word2vec method is a frequently cited method for capturing meaningful semantic relations between words from a large text corpus. It has the advantage of not requiring any tagging while training. The prevailing view is, however, that it lacks the ability to capture semantics of word sequences and is virtually useless for most purposes, unless combined with heavy machinery. This paper challenges that view, by showing that by augmenting the word2vec representation with one of a few pooling techniques, results are obtained surpassing or comparable with the best literature algorithms. This improved performance is justified by theory and verified by extensive experiments on well studied NLP benchmarks (This work is inspired by [10]).},
  file = {/Users/shirin/Zotero/storage/FE87L376/Lev et al. - 2015 - In Defense of Word Embedding for Generic Text Repr.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Fisher Information Matrix,Gaussian Mixture Model,Independent Component Analysis,Mean Average Precision},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{mahataIdentificationRankingEventSpecific2015,
  title = {Identification and {{Ranking}} of {{Event}}-{{Specific Entity}}-{{Centric Informative Content}} from {{Twitter}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Mahata, Debanjan and Talburt, John R. and Singh, Vivek Kumar},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {275--281},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_24},
  abstract = {Twitter has become the leading platform for mining information related to real-life events. A large amount of the shared content in Twitter are non-informative spams and informal personal updates. Thus, it is necessary to identify and rank informative event-specific content from Twitter. Moreover, tweets containing information about named entities (like person, place, organization, etc.) occurring in the context of an event, generates interest and aids in gaining useful insights. In this paper, we develop a novel generic model based on the principle of mutual reinforcement, for representing and identifying event-specific, as well as entity-centric informative content from Twitter. An algorithm is proposed that ranks tweets in terms of event-specific, entity-centric information content by leveraging the semantics of relationships between different units of the model.},
  file = {/Users/shirin/Zotero/storage/XPQ6WM4Z/Mahata et al. - 2015 - Identification and Ranking of Event-Specific Entit.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Affinity Matrix,Australian Prime Minister,Informative Content,Initial Score,Mutual Reinforcement},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{MAREWINTNewMaterials,
  title = {{{MARE}}-{{WINT}}: {{New Materials}} and {{Reliability}} in {{Offshore Wind Turbine Technology}}},
  url = {https://library.oapen.org/handle/20.500.12657/27774},
  urldate = {2021-02-02},
  file = {/Users/shirin/Zotero/storage/UJAVLDS2/27774.html}
}

@inproceedings{martinez-camaraImprovingSpanishPolarity2015,
  title = {Improving {{Spanish Polarity Classification Combining Different Linguistic Resources}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Martínez-Cámara, Eugenio and Cruz, Fermín L. and Molina-González, M. Dolores and Martín-Valdivia, M. Teresa and Ortega, F. Javier and Ureña-López, L. Alfonso},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {234--245},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_21},
  abstract = {Sentiment analysis is a challenging task which is attracting the attention of researchers. However, most of work is only focused on English documents, perhaps due to the lack of linguistic resources for other languages. In this paper, we present several Spanish opinion mining resources in order to develop a polarity classification system. In addition, we propose the combination of different features extracted from each resource in order to train a classifier over two different opinion corpora. We prove that the integration of knowledge from several resources can improve the final Spanish polarity classification system. The good results encourage us to continue developing sentiment resources for Spanish, and studying the combination of features extracted from different resources.},
  file = {/Users/shirin/Zotero/storage/GIFFMTDR/Martínez-Cámara et al. - 2015 - Improving Spanish Polarity Classification Combinin.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Lexicon-based approach,Polarity classification,Sentiment analysis,Sentiment feature generation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{metaisNaturalLanguageProcessing2019,
  title = {Natural {{Language Processing}} and {{Information Systems}}: 24th {{International Conference}} on {{Applications}} of {{Natural Language}} to {{Information Systems}}, {{NLDB}} 2019, {{Salford}}, {{UK}}, {{June}} 26-28, 2019, {{Proceedings}}},
  shorttitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Métais, Elisabeth and Meziane, Farid and Vadera, Sunil and Sugumaran, Vijayan and Saraee, Mohamad},
  date = {2019},
  volume = {11608},
  publisher = {{Springer International Publishing AG, Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-23281-8},
  editora = {Sugumaran, Vijayan and Saraee, Mohamad and Pandu Rangan, C. and Tygar, Doug and Terzopoulos, Demetri and Kittler, Josef and Steffen, Bernhard and Métais, Elisabeth and Meziane, Farid and Hutchison, David and Mitchell, John C. and Kleinberg, Jon M. and Naor, Moni and Hartmanis, Juris and Kanade, Takeo and Goos, Gerhard and Vadera, Sunil and Mattern, Friedemann},
  editoratype = {collaborator},
  file = {/Users/shirin/Zotero/storage/PG2JKYNX/Métais et al. - 2019 - Natural Language Processing and Information System.pdf},
  isbn = {978-3-030-23280-1},
  keywords = {Computer Applications ; Computer Science ; Natural Language Processing (NLP) ; Database Management ; Data Mining and Knowledge Discovery ; Theory of Computation ; Information Systems Applications (incl. Internet)},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{miOptimizedUyghurSegmentation2015,
  title = {Optimized {{Uyghur Segmentation}} for {{Statistical Machine Translation}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Mi, Chenggang and Yang, Yating and Dong, Rui and Zhou, Xi and Wang, Lei and Li, Xiao and Jiang, Tonghai and Osman, Turghun},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {395--398},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_36},
  abstract = {In this paper, we propose an optimized method to segment the Uyghur word. We consider the optimization as a classification problem; the features are extracted from Uyghur-Chinese bilingual corpus. Experimental results show that with our method the performance of Uyghur-Chinese machine translation improved significantly.},
  file = {/Users/shirin/Zotero/storage/2Q66LDU6/Mi et al. - 2015 - Optimized Uyghur Segmentation for Statistical Mach.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Character tagging,Data sparsity,Segmentation optimization,Uyghur segmentation,Uyghur-Chinese machine translation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@thesis{moiseevaStatisticalNaturalLanguage2020,
  title = {Statistical natural language processing methods for intelligent process automation},
  author = {Moiseeva, Alena},
  date = {2020-05-19},
  institution = {{Ludwig-Maximilians-Universität München}},
  url = {https://edoc.ub.uni-muenchen.de/26681/},
  urldate = {2021-02-02},
  abstract = {Nowadays, digitization is transforming the way businesses work. Recently, Artificial Intelligence (AI) techniques became an essential part of the automation of business processes: In addition to cost advantages, these techniques offer fast processing times and higher customer satisfaction rates, thus ultimately increasing sales. One of the intelligent approaches for accelerating digital transformation in companies is the Robotic Process Automation (RPA).  An RPA-system is a software tool that robotizes routine and time-consuming responsibilities such as email assessment, various calculations, or creation of documents and reports (Mohanty and Vyas, 2018). Its main objective is to organize a smart workflow and therethrough to assist employees by offering them more scope for cognitively demanding and engaging work. Intelligent Process Automation (IPA) offers all these advantages as well; however, it goes beyond the RPA by adding AI components such as Machine- and Deep Learning techniques to conventional automation solutions. Previously, IPA approaches were primarily employed within the computer vision domain. However, in recent times, Natural Language Processing (NLP) became one of the potential applications for IPA as well due to its ability to understand and interpret human language. Usually, NLP methods are used to analyze large amounts of unstructured textual data and to respond to various inquiries. However, one of the central applications of NLP within the IPA domain – are conversational interfaces (e.g., chatbots, virtual agents) that are used to enable human-to-machine communication. Nowadays, conversational agents gain enormous demand due to their ability to support a large number of users simultaneously while communicating in a natural language. The implementation of a conversational agent comprises multiple stages and involves diverse types of NLP sub-tasks, starting with natural language understanding (e.g., intent recognition, named entity extraction) and going towards dialogue management (i.e., determining the next possible bots action) and response generation. Typical dialogue system for IPA purposes undertakes straightforward customer support requests (e.g., FAQs), allowing human workers to focus on more complicated inquiries. In this thesis, we are addressing two potential Intelligent Process Automation (IPA) applications and employing statistical Natural Language Processing (NLP) methods for their implementation. The first block of this thesis (Chapter 2 – Chapter 4) deals with the development of a conversational agent for IPA purposes within the e-learning domain. As already mentioned, chatbots are one of the central applications for the IPA domain since they can effectively perform time-consuming tasks while communicating in a natural language. Within this thesis, we realized the IPA conversational bot that takes care of routine and time-consuming tasks regularly performed by human tutors of an online mathematical course. This bot is deployed in a real-world setting within the OMB+ mathematical platform. Conducting experiments for this part, we observed two possibilities to build the conversational agent in industrial settings – first, with purely rule-based methods, considering the missing training data and individual aspects of the target domain (i.e., e-learning). Second, we re-implemented two of the main system components (i.e., Natural Language Understanding (NLU) and Dialogue Manager (DM) units) using the current state-of-the-art deep-learning architecture (i.e., Bidirectional Encoder Representations from Transformers (BERT)) and investigated their performance and potential use as a part of a hybrid model (i.e., containing both rule-based and machine learning methods). The second part of the thesis (Chapter 5 – Chapter 6) considers an IPA subproblem within the predictive analytics domain and addresses the task of scientific trend forecasting. Predictive analytics forecasts future outcomes based on historical and current data. Therefore, using the benefits of advanced analytics models, an organization can, for instance, reliably determine trends and emerging topics and then manipulate it while making significant business decisions (i.e., investments). In this work, we dealt with the trend detection task – specifically, we addressed the lack of publicly available benchmarks for evaluating trend detection algorithms. We assembled the benchmark for the detection of both scientific trends and downtrends (i.e., topics that become less frequent overtime). To the best of our knowledge, the task of downtrend detection has not been addressed before. The resulting benchmark is based on a collection of more than one million documents, which is among the largest that has been used for trend detection before, and therefore, offers a realistic setting for the development of trend detection algorithms.},
  file = {/Users/shirin/Zotero/storage/V55ZPPR8/Moiseeva - 2020 - Statistical natural language processing methods fo.pdf;/Users/shirin/Zotero/storage/DFWLGL2D/26681.html},
  langid = {german},
  type = {Text.PhDThesis}
}

@article{nalchigarDesigningBusinessAnalytics2020a,
  title = {Designing {{Business Analytics Solutions}}},
  author = {Nalchigar, Soroosh and Yu, Eric},
  date = {2020-02-01},
  journaltitle = {Business \& Information Systems Engineering},
  shortjournal = {Bus Inf Syst Eng},
  volume = {62},
  pages = {61--75},
  issn = {1867-0202},
  doi = {10.1007/s12599-018-0555-z},
  url = {https://doi.org/10.1007/s12599-018-0555-z},
  urldate = {2021-02-01},
  abstract = {The design and development of data analytics systems, as a new type of information systems, has proven to be complicated and challenging. Model based approaches from information systems engineering can potentially provide methods, techniques, and tools for facilitating and supporting such processes. The contribution of this paper is twofold. Firstly, it introduces a conceptual modeling framework for the design and development of advanced analytics systems. It illustrates the framework through a case and provides a sample methodological approach for using the framework. The paper demonstrates potential benefits of the framework for requirements elicitation, clarification, and design of analytical solutions. Secondly, the paper presents some observations and lessons learned from an application of the framework by an experienced practitioner not involved in the original development of the framework. The findings were then used to develop a set of guidelines for enhancing the understandability and effective usage of the framework.},
  file = {/Users/shirin/Zotero/storage/F3S6KFEQ/Nalchigar and Yu - 2020 - Designing Business Analytics Solutions.pdf},
  langid = {english},
  number = {1}
}

@book{NaturalLanguageProcessing2016,
  title = {Natural {{Language Processing}} and {{Computational Linguistics}} 1},
  date = {2016},
  edition = {1},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119145554},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/9781119145554},
  urldate = {2020-11-23},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119145554},
  file = {/Users/shirin/Zotero/storage/DUNMTUM9/9781119145554.html}
}

@inproceedings{nirenburgInterplayLanguageProcessing2015,
  title = {The {{Interplay}} of {{Language Processing}}, {{Reasoning}} and {{Decision}}-{{Making}} in {{Cognitive Computing}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Nirenburg, Sergei and McShane, Marjorie},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {167--179},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_15},
  abstract = {Integrating language processing, reasoning and decision making is a prerequisite to any breakthroughs in cognitive computing. This paper discusses historical attitudes that have worked against such integration, then describes a cognitive architecture called OntoAgent that illustrates both the feasibility and the payoffs of pursuing integration. Examples are drawn from the Maryland Virtual Patient prototype application, which offers medical trainees the opportunity to diagnose and treat a cohort of cognitively modeled virtual patients that are capable of language processing, reasoning, learning, decision making and simulated action.},
  file = {/Users/shirin/Zotero/storage/LEASXEAU/Nirenburg and McShane - 2015 - The Interplay of Language Processing, Reasoning an.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Cognitive architecture,Intelligent agents,Natural language processing,Reasoning},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{nisioiUnsupervisedClassificationTranslated2015,
  title = {Unsupervised {{Classification}} of {{Translated Texts}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Nisioi, Sergiu},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {323--334},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_29},
  abstract = {In our paper we investigate the possibility to use an unsupervised classifier to automatically distinguish between the translated and original novels of a multilingual writer (Vladimir Nabokov) and to determine whether the authorship of a translated document can be achieved. We employ a rank-based document vector representation using only function words as features. To extract the results, we propose a generalization of Ward’s hierarchical clustering method that is compatible with any similarity metric.},
  file = {/Users/shirin/Zotero/storage/Y5MWKQDP/Nisioi - 2015 - Unsupervised Classification of Translated Texts.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Adjusted Rand Index,Function Word,Manhattan Distance,Rank Type,Supervise Classifier},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{nizalowskaLanguageIndependentMethodDetection2015,
  title = {A {{Language}}-{{Independent Method}} for {{Detection}} and {{Correction}} of {{Alignment Errors}} in {{Parallel Corpora}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Niżałowska, Katarzyna and Markowska-Kaczmar, Urszula},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {335--346},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_30},
  abstract = {We present a method for detection of the alignment errors in parallel corpora. The method is meant to be language-independent and was tested for pairs of English, Polish and Spanish languages. It utilizes automatically obtained dictionaries to perform the detection. A discussion about the origin of errors is included. An approach to correcting one of classes of errors is also described and tested. The proposed method has proven itself to be effective in improving the quality of Parallel Corpora. Conclusions of this study may be useful while dealing with errors in existing parallel data sources, as well as at the stage of aligning new parallel corpora.},
  file = {/Users/shirin/Zotero/storage/9DNYQQ9B/Niżałowska and Markowska-Kaczmar - 2015 - A Language-Independent Method for Detection and Co.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Corpus analysis,Data cleaning,Machine Translation,Sentence alignment},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{nouzaCrossLingualAdaptationBroadcast2018,
  title = {Cross-{{Lingual Adaptation}} of {{Broadcast Transcription System}} to {{Polish Language Using Public Data Sources}}},
  booktitle = {Human {{Language Technology}}. {{Challenges}} for {{Computer Science}} and {{Linguistics}}},
  author = {Nouza, Jan and Cerva, Petr and Safarik, Radek},
  editor = {Vetulani, Zygmunt and Mariani, Joseph and Kubis, Marek},
  date = {2018},
  pages = {31--41},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-93782-3_3},
  abstract = {We present methods and procedures designed for cost-efficient adaptation of an existing speech recognition system to Polish. The system (originally built for Czech language) is adapted using common texts and speech recordings accessible from Polish web-pages. The most critical part, an acoustic model (AM) for Polish, is built in several steps, which include: (a) an initial bootstrapping phase that utilizes existing Czech AM, (b) a lightly-supervised iterative scheme for automatic collection and annotation of Polish speech data, and finally (c) acquisition of a large amount of broadcast data in an unsupervised way. The developed system has been evaluated in the task of automatic content monitoring of major Polish TV and Radio stations. Its transcription accuracy (measured on a set of 4 complete TV news shows with total duration of 105 min) is 79,2\%. For clean studio speech, its accuracy gets over 92\%.},
  file = {/Users/shirin/Zotero/storage/JAJ45BBQ/Nouza et al. - 2018 - Cross-Lingual Adaptation of Broadcast Transcriptio.pdf},
  isbn = {978-3-319-93782-3},
  keywords = {Acoustic model training,Broadcast monitoring,Cross-lingual adaptation,Speech recognition of polish},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{oufaidaUsingDistributedWord2015,
  title = {Using {{Distributed Word Representations}} and {{mRMR Discriminant Analysis}} for {{Multilingual Text Summarization}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Oufaida, Houda and Blache, Philippe and Nouali, Omar},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {51--63},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_4},
  abstract = {Multilingual summarization task aims to develop summarization systems that are fully or partly language free. Extractive techniques are at the center of such systems. They use statistical features to score and extract most relevant sentences to form a summary within a size limit. In this paper, we investigate recently released multilingual distributed word representations combined with mRMR discriminant analysis to score terms then sentences. We also propose a novel sentence extraction algorithm to deal with redundancy issue. We present experimental results of our system applied to three languages: English, Arabic and French using the TAC MultiLing 2011 Dataset. Our results demonstrate that word representations enhance the summarization system, MeMoG and ROUGE results are comparable to recent state-of-the-art systems.},
  file = {/Users/shirin/Zotero/storage/STI5FW44/Oufaida et al. - 2015 - Using Distributed Word Representations and mRMR Di.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Discriminant analysis,Distributed word representations,Maximum relevance,Minimum redundancy,Multilingual summarization},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{pawarDecipheringReviewComments2015,
  title = {Deciphering {{Review Comments}}: {{Identifying Suggestions}}, {{Appreciations}} and {{Complaints}}},
  shorttitle = {Deciphering {{Review Comments}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Pawar, Sachin and Ramrakhiyani, Nitin and Palshikar, Girish K. and Hingmire, Swapnil},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {204--211},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_18},
  abstract = {The problem of classifying sentences into various categories, arises frequently in text mining applications. One of the most important categorization of sentences observed in product reviews, movie reviews, blogs, customer feedbacks is - Suggestions, Appreciations and Complaints. We observed that the document classification techniques do not perform well for these three non-topical sentence classes. We propose to solve this problem using a supervised approach based on Dependency-based Word Subsequence Kernel and its variations. We compare the performance of our approach with the state-of-the-art short text classification techniques on 2 different datasets - Performance Appraisal comments and Product Reviews.},
  file = {/Users/shirin/Zotero/storage/VYLH9QFM/Pawar et al. - 2015 - Deciphering Review Comments Identifying Suggestio.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Dependency Tree,Performance Appraisal,Product Review,Short Text,Word Class},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{przybylaGatheringKnowledgeQuestion2015,
  title = {Gathering {{Knowledge}} for {{Question Answering Beyond Named Entities}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Przybyła, Piotr},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {412--417},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_39},
  abstract = {This paper presents an entity recognition (ER) module for a question answering system for Polish called RAFAEL. Two techniques of ER are compared: traditional, based on named entity categories (e.g. person), and novel Deep Entity Recognition, using WordNet synsets (e.g. impressionist). The latter is possible thanks to a previously assembled entity library, gathered by analysing encyclopaedia definitions. Evaluation based on over 500 questions answered on the grounds of Wikipedia suggests that the strength of DeepER approach lies in its ability to tackle questions that demand answers beyond the categories of named entities.},
  file = {/Users/shirin/Zotero/storage/9JGBXUDK/Przybyła - 2015 - Gathering Knowledge for Question Answering Beyond .pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Entity Recognition,Name Entity Recognition,Question Answering,Word Sense,Word Sense Disambiguation},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{quarteroniNaturalLanguageProcessing2018,
  title = {Natural {{Language Processing}} for {{Industry}}},
  author = {Quarteroni, Silvia},
  date = {2018-04-01},
  journaltitle = {Informatik-Spektrum},
  shortjournal = {Informatik Spektrum},
  volume = {41},
  pages = {105--112},
  issn = {1432-122X},
  doi = {10.1007/s00287-018-1094-1},
  url = {https://doi.org/10.1007/s00287-018-1094-1},
  urldate = {2020-10-09},
  abstract = {Recently, natural language processing applications have become very popular in the industry. Examples of such applications include “semantic” enterprise search engines, document categorizers, speech recognizers and – last but not least – conversational agents, also known as virtual assistants or “chatbots”. The latter in particular are very sought-after in the customer care domain, where the aim is to complement the live agent experience with an artificial intelligence able to help users fulfil a task. In this paper, we discuss the challenges and limitations of industrial chatbot applications, with a particular focus on the “human-in-the-loop” aspect, whereby a cooperation between human and machine takes place in mutual interest. Furthermore, we analyse how the same aspect intervenes in other industrial natural language processing applications.},
  file = {/Users/shirin/Zotero/storage/5V7L5SFE/Quarteroni - 2018 - Natural Language Processing for Industry.pdf},
  langid = {english},
  number = {2}
}

@inproceedings{rambousekManagementPublishingMultimedia2015,
  title = {Management and {{Publishing}} of {{Multimedia Dictionary}} of the {{Czech Sign Language}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Rambousek, Adam and Horák, Aleš},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {399--403},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_37},
  abstract = {This paper describes the development of a multimedia dictionary writing system for the Czech Sign Language dictionary, prepared in cooperation of several institutions dedicated to the sign language research and study. The presented dictionary system takes the advantage of electronic format and strongly relies on the use of multimedia evidences. The dictionary system has to deal with large amount of video recordings of both a human narrator and digital avatar. Since the dictionary is prepared by several remotely located groups, the dictionary system provides support for complex publishing processes. The dictionary writing system is used in active preparation of the Czech Sign Language dictionary and already serves as a publisher for the dictionary data to the general public.},
  file = {/Users/shirin/Zotero/storage/B8IZXEHJ/Rambousek and Horák - 2015 - Management and Publishing of Multimedia Dictionary.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {DEB platform,Dictionary writing system,Multimedia dictionary,Sign language},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{rohrdantzVisuelleTextanalyse2010,
  title = {Visuelle Textanalyse},
  author = {Rohrdantz, Christian and Koch, Steffen and Jochim, Charles and Heyer, Gerhard and Scheuermann, Gerik and Ertl, Thomas and Schütze, Hinrich and Keim, Daniel A.},
  date = {2010-12-01},
  journaltitle = {Informatik-Spektrum},
  shortjournal = {Informatik Spektrum},
  volume = {33},
  pages = {601--611},
  issn = {1432-122X},
  doi = {10.1007/s00287-010-0483-x},
  url = {https://doi.org/10.1007/s00287-010-0483-x},
  urldate = {2021-02-02},
  abstract = {Methoden und Techniken zur automatischen Verarbeitung und inhaltlichen Erfassung großer Mengen an Textdokumenten haben in den vergangenen Jahren enorm an Bedeutung gewonnen. Während einerseits die Verfügbarkeit und der Zugang zu digitalisierten Textdokumenten bis dato in ungeahntem Maße gestiegen sind, erweist sich die Erfassung des semantischen Inhalts solcher Dokumentsammlungen als problematisch. Dem expandierenden Forschungsfeld der visuellen Textanalyse und Textvisualisierung kommt dabei eine Schlüsselrolle bei der Lösung von Problemstellungen aus der Praxis zu. Anhand aktueller Anwendungsbeispiele und einem Überblick über den Stand der Forschung erläutert dieser Artikel die vielfältigen Möglichkeiten, die sich durch visuelle Textanalyse ergeben.},
  file = {/Users/shirin/Zotero/storage/CWQS5FS6/Rohrdantz et al. - 2010 - Visuelle Textanalyse.pdf},
  langid = {german},
  number = {6}
}

@book{russellArtificialIntelligenceModern2014,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  date = {2014},
  edition = {3. ed., Pearson new international ed..},
  publisher = {{Pearson Education}},
  location = {{Harlow}},
  abstract = {Literaturverz. S. 1048 - 1078},
  isbn = {978-1-292-02420-2},
  keywords = {Artificial intelligence,Künstliche Intelligenz ; Agent; Künstliche Intelligenz},
  langid = {english},
  pagetotal = {ii+1091},
  series = {Pearson Custom Library}
}

@article{schuellerFrameworkFuerData2019,
  title = {Ein Framework für Data Literacy},
  author = {Schüller, Katharina},
  date = {2019-11-01},
  journaltitle = {AStA Wirtschafts- und Sozialstatistisches Archiv},
  shortjournal = {AStA Wirtsch Sozialstat Arch},
  volume = {13},
  pages = {297--317},
  issn = {1863-8163},
  doi = {10.1007/s11943-019-00261-9},
  url = {https://doi.org/10.1007/s11943-019-00261-9},
  urldate = {2021-02-02},
  abstract = {Digitalisierung und Datafizierung werden das Leben und Arbeiten im 21. Jahrhundertnachhaltig verändern. Daten sind die Ausgangsbasis für Wissens- bzw. Wertschöpfungals Grundlage für bessere Entscheidungen. Um systematisch Wissen bzw. Wert ausDaten zu schöpfen, ist deshalb zukünftig in allen Sektoren und Disziplinen dieFähigkeit, planvoll mit Daten umzugehen und sie im jeweiligen Kontext bewussteinsetzen und hinterfragen zu können, von entscheidender Bedeutung. Dies wird alsData Literacy bezeichnet und umfasst die Fähigkeiten, Daten auf kritische Art undWeise zu sammeln, zu managen, zu bewerten und anzuwenden. Hierfür bedarf eseines Kompetenzrahmens, d.h. eines Modells zur strukturierten Beschreibung voneffektivem Verhalten in einem gegebenen Aufgabenkontext. Er umfasst Kompetenzen,deren Definitionen und daraus abgeleitete Verhaltensindikatoren. Ein derartigerKompetenzrahmen soll alle Stufen des Wissens- bzw. Wertschöpfungsprozesses ausDaten abbilden; er soll alle Kompetenzdimensionen erfassen: (a) Wissen, (b)Fertigkeiten, (c) Fähigkeiten, (d) Motivation und (Wert-)Haltung; er soll es erlauben, dieerfassten Kompetenzen in konkrete und testbare Lern- oder Kompetenzziele zuüberführen; und er soll der die Interdisziplinarität der Aufgabe reflektieren, alsowiderspiegeln, dass neben Datenexperten auch Fachexperten, Datenschützer undDatenethiker benötigt werden. Dieser Beitrag stellt das neu entwickelte Data LiteracyFramework vor und ist eine gekürzte Fassung der Studie „Future Skills: Ein Frameworkfür Data Literacy“ (Arbeitspapier 47) des Hochschulforums Digitalisierung.},
  file = {/Users/shirin/Zotero/storage/8VQXRGDZ/Schüller - 2019 - Ein Framework für Data Literacy.pdf},
  langid = {german},
  number = {3}
}

@inproceedings{sordoExtractingRelationsUnstructured2015,
  title = {Extracting {{Relations}} from {{Unstructured Text Sources}} for {{Music Recommendation}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Sordo, Mohamed and Oramas, Sergio and Espinosa-Anke, Luis},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {369--382},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_33},
  abstract = {This paper presents a method for the generation of structured data sources for music recommendation using information extracted from unstructured text sources. The proposed method identifies entities in text that are relevant to the music domain, and then extracts semantically meaningful relations between them. The extracted entities and relations are represented as a graph, from which the recommendations are computed. A major advantage of this approach is that the recommendations can be conveyed to the user using natural language, thus providing an enhanced user experience. We test our method on texts from songfacts.com, a website that provides facts and stories about songs. The extracted relations are evaluated intrinsically by assessing their linguistic quality, as well as extrinsically by assessing the extent to which they map an existing music knowledge base. Finally, an experiment with real users is performed to assess the suitability of the extracted knowledge for music recommendation. Our method is able to extract relations between pair of musical entities with high precision, and the explanation of those relations to the user improves user satisfaction considerably.},
  file = {/Users/shirin/Zotero/storage/36I5UNK6/Sordo et al. - 2015 - Extracting Relations from Unstructured Text Source.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Dependency Tree,Music Genre,Name Entity Recognition,Relation Extraction,Type Song},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{stasAutomaticTranscriptionSubtitling2018,
  title = {Automatic {{Transcription}} and {{Subtitling}} of {{Slovak Multi}}-Genre {{Audiovisual Recordings}}},
  booktitle = {Human {{Language Technology}}. {{Challenges}} for {{Computer Science}} and {{Linguistics}}},
  author = {Staš, Ján and Viszlay, Peter and Lojka, Martin and Koctúr, Tomáš and Hládek, Daniel and Juhár, Jozef},
  editor = {Vetulani, Zygmunt and Mariani, Joseph and Kubis, Marek},
  date = {2018},
  pages = {42--56},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-93782-3_4},
  abstract = {This paper summarizes a recent progress in the development of the automatic transcription system for subtitling of the Slovak multi-genre audiovisual recordings, such as lectures, talks, discussions, broadcast news or TV/radio shows. The main concept is based on application of current and innovative principles and methods oriented towards speech and language processing, automatic speech segmentation, speech recognition, statistical modeling and adaptation of acoustic and language models to a specific topic, gender and speaking style of the speaker. We have developed a working prototype of automatic transcription system for the Slovak language, mainly designed for subtitling of various types of single- or multi-channel audiovisual recordings. Preliminary results show a significant decrease in word error rate relatively from 2.40\% to 47.10\% for an individual speaker in fully automatic transcription and subtitling of Slovak parliament speech, broadcast news or TEDx talks.},
  file = {/Users/shirin/Zotero/storage/KLGEUG6R/Staš et al. - 2018 - Automatic Transcription and Subtitling of Slovak M.pdf},
  isbn = {978-3-319-93782-3},
  keywords = {Automatic subtitling,Broadcast news,Lecture speech,Parliament speech,Speech recognition,Speech segmentation,User modeling},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{stevensPragmaticQueryAnswering2015,
  title = {Pragmatic {{Query Answering}}: {{Results}} from a {{Quantitative Evaluation}}},
  shorttitle = {Pragmatic {{Query Answering}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Stevens, Jon Scott and Benz, Anton and Reuße, Sebastian and Klabunde, Ralf and Raithel, Lisa},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {110--123},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_9},
  abstract = {This paper reports on an implementation of methods for generating indirect responses in question-answering dialogue based on domain-level strategic reasoning. User’s questions are interpreted as reflexes of underlying user requirements which are potentially satisfied by information beyond what is directly asked about. We find that the algorithms that reason about user requirements yield significantly shorter dialogues than a simpler baseline, and that users are able to interact with these systems in a pragmatically natural way.},
  file = {/Users/shirin/Zotero/storage/IB9VSHUB/Stevens et al. - 2015 - Pragmatic Query Answering Results from a Quantita.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Dialogue Move,Dialogue System,Direct Answer,Strategic Model,User Requirement},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{thanakiPythonNaturalLanguage2017,
  title = {Python Natural Language Processing: : Explore {{NLP}} with Machine Learning and Deep Learning Techniques},
  shorttitle = {Python Natural Language Processing},
  author = {Thanaki, Jalaj},
  date = {2017},
  publisher = {{Packt Publishing,}},
  location = {{Birmingham, UK :}},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&AN=1566414},
  urldate = {2020-10-13},
  abstract = {Description based on online resource; title from title page (Safari, viewed August 18, 2017).},
  isbn = {978-1-78728-552-1},
  keywords = {COMPUTERS,Data Processing,General,Machine learning,Natural Language Processing,Natural language processing (Computer science),Programming Languages,Python (Computer program language)},
  langid = {english}
}

@inproceedings{tydykovInteractiveLearningTREE2015,
  title = {Interactive {{Learning}} with {{TREE}}: {{Teachable Relation}} and {{Event Extraction System}}},
  shorttitle = {Interactive {{Learning}} with {{TREE}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Tydykov, Maya and Zeng, Mingzhi and Gershman, Anatole and Frederking, Robert},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {261--274},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_23},
  abstract = {Information extraction, and specifically event and relation extraction from text, is an important problem in the age of big data. Current solutions to these problems require large amounts of training data or extensive feature engineering to find domain-specific events. We introduce a novel Interactive Learning approach that greatly reduces the number of training examples needed and requires no feature engineering. Our method achieves event detection precision in the 80 s and 90 s with only 1 h of human supervision.},
  file = {/Users/shirin/Zotero/storage/XDW5DS9Z/Tydykov et al. - 2015 - Interactive Learning with TREE Teachable Relation.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Active Learning,Classifier Confidence,Conditional Random Field,Event Frame,Indicator Component},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{udochukwuRuleBasedApproachImplicit2015,
  title = {A {{Rule}}-{{Based Approach}} to {{Implicit Emotion Detection}} in {{Text}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Udochukwu, Orizu and He, Yulan},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {197--203},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_17},
  abstract = {Most research in the area of emotion detection in written text focused on detecting explicit expressions of emotions in text. In this paper, we present a rule-based pipeline approach for detecting implicit emotions in written text without emotion-bearing words based on the OCC Model. We have evaluated our approach on three different datasets with five emotion categories. Our results show that the proposed approach outperforms the lexicon matching method consistently across all the three datasets by a large margin of 17–30 \% in F-measure and gives competitive performance compared to a supervised classifier. In particular, when dealing with formal text which follows grammatical rules strictly, our approach gives an average F-measure of 82.7 \% on “Happy”, “Angry-Disgust” and “Sad”, even outperforming the supervised baseline by nearly 17 \% in F-measure. Our preliminary results show the feasibility of the approach for the task of implicit emotion detection in written text.},
  file = {/Users/shirin/Zotero/storage/YBZUBX49/Udochukwu and He - 2015 - A Rule-Based Approach to Implicit Emotion Detectio.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Emotion detection,Implicit emotions,OCC model,Rule-based approach},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{veidlingerDigitalHumanitiesBuddhism2019,
  title = {Digital {{Humanities}} and {{Buddhism}}: {{An Introduction}}},
  shorttitle = {Digital {{Humanities}} and {{Buddhism}}},
  author = {Veidlinger, Daniel},
  date = {2019},
  publisher = {{De Gruyter, Inc}},
  location = {{Berlin/Boston}},
  isbn = {978-3-11-051836-8},
  keywords = {Digital humanities},
  langid = {english}
}

@inproceedings{veresHowTalkCognitive2015,
  title = {How to {{Talk}} to a {{Cognitive Computer}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Veres, Csaba},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {153--159},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_13},
  abstract = {Cognitive Computing is becoming a catchphrase in the technology world, with the promise of new smart services offered by industry giants like IBM and Google. We observe that the latest technologies do not represent a major departure from previous achievements in Artificial Intelligence. An example from language processing demonstrates that present day Cognitive Computing still struggles with long-standing issues in AI. We conclude that in the absence of fundamental breakthroughs, it might be more fruitful to follow Licklider’s lead and adopt Symbiotic Computing as a metaphor for designing software programs that enhance human cognitive performance.},
  file = {/Users/shirin/Zotero/storage/W6REBKKY/Veres - 2015 - How to Talk to a Cognitive Computer.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {AI,Cognition,Cognitive computing,Language,Symbiosis},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{weissenbacherTreeStructuredNamedEntities2015,
  title = {Tree-{{Structured Named Entities Extraction}} from {{Competing Speech Transcriptions}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Weissenbacher, Davy and Raymond, Christian},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {249--260},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_22},
  abstract = {When real applications are working with automatic speech transcription, the first source of error does not originate from the incoherence in the analysis of the application but from the noise in the automatic transcriptions. This study presents a simple but effective method to generate a new transcription of better quality by combining utterances from competing transcriptions. We have extended a structured Named Entity (NE) recognizer submitted during the ETAPE Challenge. Working on French TV and Radio programs, our system revises the transcriptions provided by making use of the NEs it has detected. Our results suggest that combining the transcribed utterances which optimize the F-measures, rather than minimizing the WER scores, allows the generation of a better transcription for NE extraction. The results show a small but significant improvement of 0.9 \% SER against the baseline system on the ROVER transcription. These are the best performances reported to date on this corpus.},
  file = {/Users/shirin/Zotero/storage/Z5UH4NCS/Weissenbacher and Raymond - 2015 - Tree-Structured Named Entities Extraction from Com.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Multi-pass decoding,Speech transcription,Structured named entities},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wiegandCombiningPatternBasedDistributional2015,
  title = {Combining {{Pattern}}-{{Based}} and {{Distributional Similarity}} for {{Graph}}-{{Based Noun Categorization}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Wiegand, Michael and Roth, Benjamin and Klakow, Dietrich},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {64--72},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_5},
  abstract = {We examine the combination of pattern-based and distributional similarity for the induction of semantic categories. Pattern-based methods are precise and sparse while distributional methods have a higher recall. Given these particular properties we use the prediction of distributional methods as a back-off to pattern-based similarity. Since our pattern-based approach is embedded into a semi-supervised graph clustering algorithm, we also examine how distributional information is best added to that classifier. Our experiments are carried out on \textbackslash (5\textbackslash ) different food categorization tasks.},
  file = {/Users/shirin/Zotero/storage/3XITRMIS/Wiegand et al. - 2015 - Combining Pattern-Based and Distributional Similar.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Food Item,Graph Cluster,Neighbour Classifier,Relation Extraction,Unconnected Node},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{wiriyathammabhumComputerVisionNatural2016,
  title = {Computer {{Vision}} and {{Natural Language Processing}}: {{Recent Approaches}} in {{Multimedia}} and {{Robotics}}},
  shorttitle = {Computer {{Vision}} and {{Natural Language Processing}}},
  author = {Wiriyathammabhum, Peratham and Summers-Stay, Douglas and Fermüller, Cornelia and Aloimonos, Yiannis},
  date = {2016-12-12},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {49},
  pages = {71:1--71:44},
  issn = {0360-0300},
  doi = {10.1145/3009906},
  url = {http://doi.org/10.1145/3009906},
  urldate = {2020-10-09},
  abstract = {Integrating computer vision and natural language processing is a novel interdisciplinary field that has received a lot of attention recently. In this survey, we provide a comprehensive introduction of the integration of computer vision and natural language processing in multimedia and robotics applications with more than 200 key references. The tasks that we survey include visual attributes, image captioning, video captioning, visual question answering, visual retrieval, human-robot interaction, robotic actions, and robot navigation. We also emphasize strategies to integrate computer vision and natural language processing models as a unified theme of distributional semantics. We make an analog of distributional semantics in computer vision and natural language processing as image embedding and word embedding, respectively. We also present a unified view for the field and propose possible future directions.},
  file = {/Users/shirin/Zotero/storage/YTABAAN3/Wiriyathammabhum et al. - 2016 - Computer Vision and Natural Language Processing R.pdf},
  keywords = {computer vision,distributional semantics,image captioning,image embedding,imitation learning,Language and vision,lexical semantics,multimedia,natural language processing,robotics,semantic parsing,survey,symbol grounding,visual attribute,word embedding,word2vec},
  number = {4}
}

@article{wittumAutomatedMethodsComparison2020,
  title = {Automated Methods for the Comparison of Natural Languages},
  author = {Wittum, Gabriel and Hoffer, Michael and Lemke, Babett and Jabs, Robert and Nägel, Arne},
  date = {2020-05-18},
  journaltitle = {Computing and Visualization in Science},
  shortjournal = {Comput. Visual Sci.},
  volume = {23},
  pages = {7},
  issn = {1433-0369},
  doi = {10.1007/s00791-020-00325-2},
  url = {https://doi.org/10.1007/s00791-020-00325-2},
  urldate = {2021-02-02},
  abstract = {Starting from the general question, if there is a connection between the mathematical capabilities of a student and his native language, we aim at comparing natural languages with mathematical language quantitatively. In [20] we set up an approach to compare language structures using Natural Language Processors (NLP). However, difficulties arose with the quality of the structural analysis of the NLP used just comparing simple sentences in different but closely related natural languages. We now present a comparison of different available NLPs and discuss the results. The comparison confirms the results from [20], showing that current NLPs are not capable of analysing even simple sentences such that resulting structures between different natural languages can be compared.},
  file = {/Users/shirin/Zotero/storage/D36857JS/Wittum et al. - 2020 - Automated methods for the comparison of natural la.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{zayedHybridApproachExtracting2015,
  title = {A {{Hybrid Approach}} for {{Extracting Arabic Persons}}’ {{Names}} and {{Resolving Their Ambiguity}} from {{Twitter}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Zayed, Omnia H. and El-Beltagy, Samhaa R.},
  editor = {Biemann, Chris and Handschuh, Siegfried and Freitas, André and Meziane, Farid and Métais, Elisabeth},
  date = {2015},
  pages = {355--368},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-19581-0_32},
  abstract = {Tweets offer a novel way of communication that enables users all over the world to share real-time news and ideas. The massive amount of tweets, generated regularly by Arabic speakers, has resulted in a growing interest in building Arabic named entity recognition (NER) systems that deal with the informal colloquial Arabic. The unique characteristics of the Arabic language make Arabic NER a challenging task, which, the informal nature of tweets further complicates. The majority of previous works addressing Arabic NER were concerned with formal modern standard Arabic (MSA). Moreover, taggers and parsers were often utilized to solve the ambiguity problem of Arabic persons’ names. Although, previously developed approaches perform well on MSA text, they are not suited for colloquial Arabic. This paper introduces a hybrid approach to extract Arabic persons’ names from tweets in addition to a way to resolve their ambiguity using context bigram patterns. The introduced approach attempts not to use any language-dependent resources. Evaluation of the presented approach shows a 7 \% improvement in the F-score over the best reported result in the literature.},
  file = {/Users/shirin/Zotero/storage/DZL7URU6/Zayed and El-Beltagy - 2015 - A Hybrid Approach for Extracting Arabic Persons’ N.pdf},
  isbn = {978-3-319-19581-0},
  keywords = {Arabic Language,Baseline System,Conditional Random Field,Name Entity Recognition,Training Dataset},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}


