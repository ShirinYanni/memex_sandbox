{
    "czerepowickaSEJFGrammaticalLexicon2018": {
        "0003": {
            "matches": 3,
            "pathToPage": "./_data/c/cz/czerepowickaSEJFGrammaticalLexicon2018/pages//0003.html",
            "result": "SEJF - A Grammatical Lexicon of Polish Multiword Expressions 61\n\nThe description of a MWE in Toposiaw is a multistage procedure. Firstly,\nthe lexicographer assigns the MWE to the appropriate morphosyntactic class\nequivalent to one of the 33 flezemes (inflectionally motivated POSs) used in\nthe NKJP corpus. Secondly, the MWE is segmented into words and separators,\nwhereas the latter are considered full-Hedged components that can further be\nreferred to im inflection graphs. Thirdly, each component word is automatically\nassigned a list of all lemmas and morphological tags stemming from Morfeusz,\nthus all possible homonyms are distinguished. The lexicographer manually dis-\nambiguates each word by choosing the right interpretation. Figure | shows the\nnominal MWE adwocat diabla ‘devil's advocate’, which has been segmented\ninto three components, including a space. The first component is marked by the\nlexicographer as admitting inflection. The last one obtains four morphological\ninterpretations, the third of which is correct.\n\n \n\n \n\n \n\n \n\nChoose the correct tag:\nsubstisggerint\n\n   \n\n \n\nFig. 1. Segmentation and morphosyntactic annotation of the nominal MWE adwokat\ndiabla ‘devil’s advocate’ in Toposlaw. The following codes are used: accusative case\n(acc), genitive case (gen), masculine animate gender (m2), masculine human gender\n(mi), singular (sg), space (sp), and substantive (subst).\n\n  \n\nIn the last step, the lexicographer manually chooses an existing inflection\ngraph (or creates a new one if needed) describing inflected forms of the cur-\nrent MWE entry. Figure 2 shows the inflection graph NC~O_N (cf. Table 2 for the\nmeaning of the NC, 0 and N codes) for the entry from Fig. 1. Graph paths are\napplied from left to right and the numbered boxes in them correspond to con-\nstituents. The formulae inside boxes consist of constituents’ indexes and equa-\ntions on morphological constants and variables. These equations impose con-\nstraints on the inflection, variation and agreement of constituents. Here, the for-\nmula ($1:Case=$c;Nb=$n) says that the first component (here: adwokat) inflects\nfreely for case and number. The formulae appearing below paths determine the\nfeatures of the inflected forms of the whole MWE as a function of the fea-\ntures of its constituents. Here, each form resulting from the unique path inherits\nits gender from the first constituent and has the conforming case and number\n(($1:Gen=$1. Gen; Case=8c;Nb=$n)). Variables like $c or $n are freely defined\nby the user and subject to unification, ie. if they reoccur on the same path the\nrespective components must agree (cf. Sect.5 and Fig. 4).\n\f"
        },
        "0005": {
            "matches": 6,
            "pathToPage": "./_data/c/cz/czerepowickaSEJFGrammaticalLexicon2018/pages//0005.html",
            "result": "SEJF - A Grammatical Lexicon of Polish Multiword Expressions 63\n\nOn average, compound nouns have over 12 inflected forms ~ most of them\ninflect. for case (with 7 case values) and some inflect for number (2 values).\nCompound adjectives are much more productive, with as many as almost 100\ninflected forms on average, due to the case, number and gender inflection (with\n9 gender values -- 3 masculine, 1 feminine, 2 neuter and 3 plurale tantum ones\n\naccording to the Morfeusz tagset). Compound adverbs do not inflect, while\namong other compounds ~ selected conjunctions, particles and numerals ~ only\nthe last ones inflect. The inflection graphs are mostly rather simple: 152 of them\ncontain only one path representing inflection and, possibly, agreement of com-\nponents. Eight remaining graphs (assigned to 154 MWEs in total) contain two\npaths, which account mainly for a flexible word order. Table2 shows the most\nfrequently assigned inflection graphs, the corresponding syntactic structures and\nexamples of the assigned entries. A large majority of them consists of a noun\nand an agreeing adjective in both orders.\n\n \n\nTable 2. Distribution of the most frequently assigned inflection graphs. The following\ncodes are used: nominal compound (NC), variable component (0), invariable component.\nve (Adj).\n\n \n\n(N), substantive (S), substantive in genitive (Sgen), and adje\n\n \n\nGraph / Syntactic Comment MWE examples Assigned\nstructure MWEs\n\nNC-O_O-i+ $ Adj Inflection for number — kovi trojarisht 1,153\n“Trojan horse’\n\nNC-O_O-1 Adj S Inflection for number | aksamitna rewolucja 556\n‘velvet. revolution’\n\nNC-O_O-2t = S Adj Fixed number dobra osobiste 426\n‘personal belongings’\n\nNC-O_O-it Adj S$ Fixed number czarna, magia 396\n“black magic’\n\nNC-O_N S$ Sgen Inflection for number | adwokat diabia 351\n\n‘devil’s advocate’\n\n5 Interesting Problems\n\nThe Toposlaw suite allows to successfully encode most of the nominal Polish\nMWEs however not all of them. For instance masculine human gender nouns\nare challenging in the sense that they exhibit not only the regular case and\ngender inflection but also have alternative depreciative forms in plural which\nare stylistically marked and show the speaker's pejorative attitude to the per-\nsons named by the multiword noun. Grammatically speaking, depreciative forms\ndiffer from the regular ones in plural nominative and vocative, namely they\ntake the masculine animate gender m2 (e.g. adwokaty instead of adwokaci ‘advo-\ncates’). Because of the unusual gender, these forms constitute a separate flexeme\n\f"
        },
        "0006": {
            "matches": 4,
            "pathToPage": "./_data/c/cz/czerepowickaSEJFGrammaticalLexicon2018/pages//0006.html",
            "result": "64 M. Cacrepowicka and A. Savary\n\n(of type depr, cf. the NKJP tagset®). Since Toposlaw does not. currently allow to\ngather several flexemes of the same lemma in one lexeme, generating depreciative\nforms for masculine human nominal compounds (e.g. adwokaty diabla ‘devil’s\nadvocates’) is blocked.\n\nAnother reason of a deficient description of the mflection paradigms is the\n(inevitable) incompleteness of Morfeusz. Neologisms such as rozporkowy (relative\nadjective for a trousers’ fly) are not encoded, therefore compounds such as afera\nrozporkowa (lit. fly affair) ‘a sexual scandal’ cannot be automatically inflected.\n\nChallenging examples which Topostaw allows us to cover include variable\nword order, as in aufomatyczna sekretarka, sekretarka automatyczna (lit. auto-\nmatic secretary) ‘answering machine’, or fluctuation of the grammatical gender.\n\n \n\n \n\nis\n\n \n\nFor instance, the nominal unit czerwony pajak (lit. red spider) ‘communis\nexocentric in that its noun component pajak ‘spider’ is in masculine human\nanimate gender (m2), while the whole compound, denoting a person, has the\nmasculine human (m1) behavior. As shown on the upper path in Fig. 4, while the\ncase and number of the whole MWE are conforming to the ones of the (inflected)\nnoun and adjective, it’s gender is not inherited from component 3 but given by\nthe constant value m1. The major difference in inflection paradigms of masculine\nhuman and animate nouns is in the plural accusative form. It is equal to the\nplural genitive for m1 (czerwonych pajakéw) and to the plural nominative for m2\n(ezerwone pajaki). The second path in Fig. 4 accounts for the m2-to-m1 shift: the\naccusative plural masculine human form of the whole compound is obtained by\ncombining the genitive rather than the accusative forms of the two components.\nThe inflection paradigm generated by the graph in Fig. 4 for czerwony pajak is\nshown in Fig. 5.\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nezerwony pajak\n“$LiCase= $oiNb=gn> | <$2> } <$3iCase= Fe -Nbxgn>\n\n__V <Case=$c;Nb=$¢n;Gen=m1> a\nczerwony pajak\n\n \n\n<$1:Case=gen:;Nb=pl> | <g2> |} <$9:Case=gen;Nb=pl>\n<Casexacc;Nb=plGen=m1>\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 4. Inflection graph NC~O_N describing a masculine gender fluctuation in czerwony\npajak (lit. red spider) ‘communist’.\n\n6 Evaluation\n\nIn order to perform an evaluation of the lexicon we prepared a corpus of general\nPolish language texts manually annotated with contiguous MWEs. It consists\nof documents extracted from the manually annotated subcorpus of the National\nCorpus of Polish. This subcorpus does not contain full texts but only randomly\nselected paragraphs thereof, and for the sake of our evaluation we chose the\n\n5 http://nkjp.pl/poligarp/help/ense2.html.\n\f"
        },
        "0010": {
            "matches": 1,
            "pathToPage": "./_data/c/cz/czerepowickaSEJFGrammaticalLexicon2018/pages//0010.html",
            "result": "68 M. Cacrepowicka and A. Savary\n8 Related Work\n\nAlthough MWEs are still under-represented in language resources and tools,\nefforts have been put towards bridging this gap from the e-lexicographical point\nof view in many languages, as discussed in [15]. The community around Intex’,\nNooJ® and Unitex has a long e-lexicographic tradition related to compounds,\nwith dictionaries of compounds created for French [28], English [24], Greek [14]\nand others. Lexicons similar to SEJF, following the Multiflex paradigm, exist or\nare under construction for Serbian [13], Greek [9], and Macedonian [22]. Various\ne-lexicographic frameworks were developed for the creation of MWE e-lexicons\nnotably in Turkish [18], Basque [2], Dutch [11], Serbian [29] and Hebrew [1], the\nlast one also covers verbal MWEs.\n\nOn the Polish ground, SEJF is one of three grammatical lexicons of Polish\nmultiword units built under Toposlaw. The two other resources are: (i) SAWA®\n[17], a grammatical lexicon of Warsaw urban proper names (streets, squares,\nbus stops, and other objects linked to the Warsaw communication network), (ii)\nSEJFEK\"® [27], a grammatical lexicon of Polish economic terminology contain-\ning over 11,000 specialized nominal compounds. Complementary formalisms for\ninflectional paradigms of Polish MWEs have been presented in [5, 10].\n\n \n\n9 Conclusions and Future Work\n\nWe have presented the construction of SEJF, an electronic grammatical lexi-\ncon of Polish nominal, adjectival and adverbial MWEs. It is one of the first\nsteps towards a systematic and extensive description of such units, applicable to\nautomatic text processing in Polish, including richly annotated corpora such as\ntreebanks, While the coverage of compound adverbs offered by SEJF is reason-\nable, its contents in terms of compound nouns and adjectives should be extended,\nas shown by the evaluation results. Additional corpora can underlie this farther\nwork, including those available via Sketch Engine!! with collocation support [21].\n\nAs mentioned in Sect.5 the description of nominal MWEs in masculine\nhuman gender is not fully satisfactory with Toposlaw, due to the impossibil-\nity to generate the depreciative forms of these expressions. These problems can\nbe overcome with a recent follow-up of Toposiaw, called Werbosiaw, which allows\nthe user to gather several flexemes of the same lemma in one lexeme.\n\nMore precisely, according to [23], a lexeme is understood as an abstract\nunit of language containing all forms connected with the same lexical meaning.\nFor instance, adwokaci diabla ‘devil’s advocates’ in human masculine (a1) and\nadwokaty diabla ‘devil’s advocates (depr.)’ in human animate (m2), belong to the\n\n \n\n  \n\n   \n\n7 http: / /intex.univ-feomte.fr/.\n\n5 http: //www.noojdnip.net/pages/nooj.html.\n\n® http: //zilipipan.waw.pl/SAWA.\n\n‘© http: //ailipipan.waw.pl/SEJFEK,\n\n4 https: //www.sketchengine.co.uk/pltenten-corpus/.\n\n \n\n  \n\n \n\f"
        },
        "0011": {
            "matches": 3,
            "pathToPage": "./_data/c/cz/czerepowickaSEJFGrammaticalLexicon2018/pages//0011.html",
            "result": "SEJF - A Grammatical Lexicon of Polish Multiword Expressions 69\n\nsame lexeme. A lexeme can further subdivide into several flexemes [4], ie. mor-\nphosyntactically homogeneous sets of forms belonging to the same lexeme. Since\na substantive (subst) is defined in the NKJP-Morfeusz tagset as a class which\ninflects for case an number and has (invariable) gender, adwokaty/adwokaci\ndiabla in two different genders cannon belong to the same nominal flexeme.\nThus, only the forms in m1 are classified into the flexeme of class subst. The\nforms in m2 are separated in another flexeme of class depr (depreciative form),\ndefined as inflecting for case and having number and gender.\n\nToposlaw is flexeme-oriented, therefore these two flexemes would have to\nbe described separately, which would be unnatural, since they both share the\nsame lemma. adwokat diabla ‘devil’ advocate’. Werbostaw, conversely, is lexeme-\noriented. Each of its individual entries is a lexeme whose class has to be selected\nby the lexicographer in the initial stage of the description, as shown in Fig. 6.\n\n   \n\n \n\n  \n\nLexicon Edit [mpart Export Options View Language Help\n\n  \n\nUnda tede}\neo General description at-antry:\nLexeme Ula\nadwrokat diabla\n\n     \n\nMe\n\n   \n \n \n\nlexeme clase: Rzt\n\nLexeme mean\nComments.\n\n \n\n \n\nFig. 6. Selecting the class (RZECZOWNIK ‘noun’, VERB, etc.) of the lexeme adwokat\ndiabta ‘devil’s advocate’ in Werboslaw.\n\nA lexeme is a unit of a higher order as compared to a flexeme. Therefore,\nthe next step is to define the list of fexemes associated with a given lexeme, as\nshown in Fig. 7.\n\n   \n\n‘Select flexenies that comprise the laxers\n\nText In le.) Graph\nwi) INC-ON depre\n\nng the flexemes (here: depr and subst) associated with the lexeme (here\nRZECZOWNIK ‘noun’) adwokat diabla ‘devil’s advocate’ in Werboslaw.\n\n  \n\n \n\nThe description of each of the flexemes follows the same steps as in Toposlaw,\nLe. consists of analyzing each component morphosyntactically and selecting the\nright inflection graph. Fig.8 shows the graph for the depreciative flexeme of\nadwokat diabla ‘devil’s advocate’. Recall that the depreciative forms only show\nin the nominative and vocative case in plural, i.e. Morfeusz only generates these\n\f"
        },
        "0012": {
            "matches": 3,
            "pathToPage": "./_data/c/cz/czerepowickaSEJFGrammaticalLexicon2018/pages//0012.html",
            "result": "70 M. Cacrepowicka and A. Savary\n\ntwo forms for a depreciative noun. Therefore, the number in the graph can be\nfixed to plural (Nb=p1) and the case inflection can be unrestricted (Case=$c).\n\nadwokaty diabla\n\noo <$1:Case=$c;Nb=pi> <$2> <$3> OQ)\n\n<$1:Case=$¢;Nb=pi;Gen=$1.Gen>\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 8. Inflection graph NC-O_N_depr2 describing the depreciative flexeme adwokaty\ndiabla ‘devil’s advocates’ of the nominal lexeme adwokat diabla ‘devil's advocate’.\n\nAs a result, the full description of the lexeme yields an enhanced list of the\ninflected forms shown in Fig. 9. Note the occurrence of the two depreciative forms\nin the m2 gender, as opposed to the paradigm obtained with Topostaw in Fig. 3.\n\n \n\nadwokat diabla & adwokat diabla:subst:sg:nom:m1\nadwokaci diabta 62 adwokat diabta:subst:pi:inom:m1\nadwokata diabla (© adwokat diabla:subst:sg:gen:mt\nadwokatow diabta 6 adwokat diabta:subst:pl:gen:m1\nadwokatowi diabla © adwokat diabla:subst:se:dat:m1\nadwokatom diabta & adwokat diabta:subst:pl:dat:mt\nadwokata diabla 62 adwokat diabta:subst:sg:acc:m1\nadwokatéw diabla & adwokat diabta:subst: :\nadwokatem diabta & adwokat diabta:subst:sg:\nadwokatami diabla &! adwokat diabla:subst:\nadwokacie diabla & adwokat diabta:subst:s:\nadwokatami diabla & adwokat diabta:subst:\nadwekacie diabta G adwokat diabta:subst:\nadwokaci diabla & adwokat diabla:subst:pl:voc:m1\nadwokaty diabla tz) adwokat diabla:subst:pt:\nadwokaty diabla G1 adwokat diabta:subst:\n\n   \n  \n\n \n\nFig. 9. Inflection paradigm of the nominal lexeme adwokat diabla ‘devil’s advocate’,\ncontaining regular and depreciative forms.\n\nAn even more challenging behavior is exhibited by Polish verbs, where a\nsingle lexeme consists of up to 12 different flexemes. For instance, the non-\npast flexemes (fin) like robi ‘does’ inflect for number and person, and have\naspect; the past flexeme (praet) like robil ‘did’ inflect for number, gender and\nagglutination, and have aspect; the gerunds (ger) like robienie ‘doing’ inflect\nfor number, case and negation, and have gender and aspect; etc. Thanks to\nflexeme-to-lexeme shift operated in Werboslaw, verbal mutiword expressions,\nsuch as odwracaé kota ogonem (lit. to turn the cat with its tail to the front) ‘to\ndistort the facts’, can now be conveniently described. Such expressions are being\ncurrently addressed within the Verbel project?? [7].\n\n'2 http://awm.edu.pl/verbel.\n\f"
        }
    },
    "stasAutomaticTranscriptionSubtitling2018": {
        "0001": {
            "matches": 1,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0001.html",
            "result": " \n\nAutomatic Transcription and Subtitling\nof Slovak Multi-genre Audiovisual\nRecordings\n\nJan Stas™), Peter Viszlay, Martin Lojka, Tomas Kocttr, Daniel Hlddek,\nand Jozef Juhar\n\nDepartment. of Electronics and Multimedia Communications, Faculty of Electrical\nEngineering and Informatics, Technical University of Kodic\nPark Komenského 13, 042 00 KoSice, Slovak Republic\n{jan.stas,peter.viszlay,martin.lojka,tomas.koctur,\ndaniel .hladek, jozef .juhar}@tuke.sk\n\n     \n\n \n\n \n \n  \n \n\nAbstract. This paper summarizes a recent progress in the development\nof the automatic transcription em for subtitling of the Slovak multi-\ngenre audiovisual recordings, such as lectures, talks, discussions, broad-\ncast news or TV/radio shows. The main concept is based on appl\ntion of current and innovative principles and methods oriented towards\nspeech and language processing, automatic speech segmentation, speech\nrecognition, statistical modeling and adaptation of acoustic and language\nmodels to a specific topic, gender and speaking style of the speaker. We\nhave developed a working prototype of automatic transcription system\nfor the Slovak language, mainly designed for subtitling of various types of\nsingle- or multi-channel audiovisual recordings. Preliminary results show\na significant decrease in word error rate relatively from 2.40% to 47.10%\nfor an individual speaker in fully automatic transcription and subtitling\nof Slovak parliament speech, broadcast news or TEDx talks.\n\n     \n\n  \n\nam\n\n   \n\n    \n\n \n\n  \n\n \n\nKeywords: Automatic subtitling - Broadcast news - Lecture speech\nParliament speech - Speech recognition - Speech segmentation\nUser modeling\n\n1 Introduction\n\nThe development of the automatic transcription systems for subtitling of various\nmulti-genre audiovisual recordings is very popular research area, when human-\nmachine interaction (HCI) through speech recognition technologies is becoming\nmore and more integrated into the everyday use.\n\nThe TV broadcasters are interested in automated transcription of audiovisual\nrecordings, such as academic lectures, talks, discussions [1], or broadcast news\n(BN) [5], because of the valid EU governments regulations watching the amount\nof TV shows with closed captions for hearing impaired in each member state.\n\n \n\n \n\nNature 2018\n, 2018.\n\n    \n\n© Springer International Publishing AG, part of Springe\nZ. Vetulani et al. (Eds.): LPC 2015, LNAI 10930, pp. 42-!\nht :/ /dolorg/ 10.1007 /978-3-319-93782-3 4\n\n \n\n   \n\n \n\f"
        },
        "0002": {
            "matches": 1,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0002.html",
            "result": "Aut. Transcript. and Subtitl. of Slovak Multi-genre AV Recordings 43\n\nThe European Federation of Hard of Hearing People (EFHOH) in its annual\nreport “Creating @ barrier-free Europe for all hard of hearing citizens” from\nyear 2011 [8], addressed to the European Parliament, pushes ahead the idea to\nincrease the ratio of programs accompanied by open or closed captions to 100%\nuntil year 2020 in each member state of the EU. This goal has already been\nreached in countries like Great Britain, Netherlands or France.\n\nFor Slovakia, the mininvum amount is 10% for commercial and 50% for public\nbroadcasters and this is usually achieved by manual transcriptions. That is why\nthe global companies try to present products in this area, which usually work\nsatisfactory for major languages, but they are insufficient for the minor ones as\nis Slovak. As a result, broadcasters and subtitling companies are seeking for\nsubtitling alternatives more productive than the traditional manual process [2].\n\nCurrent trend in the area of the creation of closed captions is utilization of\nthe automatic speech recognition and application of the modern principles and\nmethods of the speech technologies in automatic transcription of speech in real\ntime. The FP7 project SAVAS? provide such applications for major EU lan-\nguages, as is English, French, German, Italian, Spanish, or Portuguese. Similar\nproject of applied R&D for automatic subtitling was solved by the Universi\nof Western Bohemia in Pilsen for the Czech Television during research project\nELJABR? solved in years 2006-2011 and the project ELJABR II that started in\nyear 2011 and is still running. Several significant results in automatic subtitling\nof broadcast news were reported for the Slovenian [14] or Hungarian [22] lan-\nguage. Therefore, we have oriented our research on the development of a working\nprototype of the automatic subtitling system for the Slovak language.\n\nThe Slovak transcription system allows multi-channel automatic segmenta-\ntion of the speech recordings at several levels based on the decomposition using\nprincipal component analysis with possible gender and speaker detection. It can\nbe used for subtitling of various types of single- or multi-channel audiovisual\nrecordings. The system makes use of multi-pass sequential speech recognition\nrunning on a computational server for each channel using a combination of mul-\ntiple parallel speech recognition systems with different settings for general and\nspecifically adapted acoustic and language models with hypothesis combination.\n\nIn the following sections we will describe the main components of the pro-\nposed automatic transcription system for subtitling of Slovak multi-genre audio-\nvisual recordings in more detail.\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n2 Web-Based User Interface\n\nThe web-based user interface offers to upload and automatically process mul-\ntimedia files. The user can select one of two methods for digital audio signal\nprocessing: single- or multi-channel automatic speech segmentation (see Fig. 1).\n\n' hetp://googleblog.blogspot.sk/2009/11/automatic\n* netp:/ /www-fp7-savas.eu/.\n* hetp://www.kky.zcu.cz/en/research-fields/eljabr.\n\ncaptions-in- youtube. html.\n\n \n\n \n\f"
        },
        "0004": {
            "matches": 6,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0004.html",
            "result": "Aut. Transcript. and Subtitl. of Slovak Multi-genre AV Recordings 45\n\nSpeech segments are sent to the multi-threaded parallel speech recognition\nserver according to actual server’s capacity. The advantage of using speech recog-\nnition server is simultaneous speech recognition of multiple speech segments.\nParallel speech recognition of each media is monitored automatically and servers\nload is dynamically allocated. To reduce the load on recognition server, audio\nsegments are sent only when the server has enough capacity to recognize seg-\nment. Individual segments are then transcribed with appropriate configuration\nfor speech recognition with a different setup for male and female gender, or indi-\nvidual speaker. If speaker’s gender is not recognized by the gender detection (see\nSect.3), the general configuration for speech recognition with the background\nacoustic model (AM) and language model (LM) is used.\n\nSpeech recognition text output obtained from recognition server is time\naligned to segmentation timestamps, post-processed to generate subtitles in\nWebVTT format, supported by HTMLS5. All subtitles are stored in WebVTT\nfile and also in MySQL database for future processing, correction, and indexing.\n\nSince HTMLS5 does not support MKV format, MK’V video files are converted\nto MP4 format with 2 streams (audio/video), for further subtitling, archiving and\nindexing. The first stream is an original video stream without audio track and the\nsecond one is jomed audio stream converted to AAC format. Selected media can\nbe played with standard subtitles or subtitles as “karaoke”. A simple web-based\ngraphical user interface® (see Fig. 2) has been designed for this purpose [9].\n\n3 Automatic Speech Segmentation\n\nThe accurate automatic speech transcription of an unknown audio stream\ndepends on several processing steps that have to be performed before the main\nrecognition. In the presented system, these steps are covered by speaker diariza-\ntion, robust and discriminative feature extraction and finally, by buildg gender-\nand speaker-dependent AMs for gender detection and speaker identification pur-\nposes. In the recognition phase, a precise automatic speech segmentation is\nemployed to identify voice/speech active segments that are additionally labelled\nwith gender- and speaker-specific labels.\n\n \n\n \n\n3.1 Speaker Diarization\n\nThe speaker diarization is an automated annotation of speech recordings with\nlabels that represent speakers. This task is performed without any prior infor-\nmation, neither the number of speakers, nor their identities, nor samples of their\nvoices are available. The speaker diarization can be also employed for helping\nspeech recognition, facilitating the searching and indexing of audio archives and\nincreasing the richness of automatic transcriptions [17].\n\nThe diarization usually consists of voice activity detection (VAD), in better\ncase speech activity detection (SAD) and feature extraction. Speech segments\n\n   \n\n® hnttp://isada.kemt.fei.tuke.sk/.\n\f"
        },
        "0005": {
            "matches": 4,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0005.html",
            "result": "46 J. Stas et al.\n\nGMM-based AM GMM-based AM\na\n\nspeakers\n\n  \n   \n \n\n \n\nmulti-channel\n\n   \n\n  \n   \n  \n  \n\n \n  \n   \n   \n \n\n     \n   \n\n     \n \n\n   \n   \n  \n\n  \n\n  \n \n\nwaveform\ni 4st level 2nd level time stamps\n7 32.98 49.46 ch? female\n\nia CD-2DLDA precise 50.15 56.22 chi male\namine | cgmenmtan Fo] HLA Lond “Pr sera Ly Si ON oY Si\n\n—_ feature wre) detection 73.36 6235 chi cnale\nmelt dot container-like bufter| extraction MIF detection 9237 8607 nz Fem,\netc\n\nFig. 3. Block diagram of the proposed automatic speech segmentation\n\nbelonging to the same speaker are clustered together by hierarchical agglomera-\nive clustering. The Viterbi decoding (re-segmentation) is performed to generate\na new segmentation realigned on the speaker boundaries.\nIn our framework, the LIUM SpkDiarization Toolkit [8,17] was successfully\napplied for the imitial segmentation and speaker clustermg. The diarization\nsroduced homogeneous segments belonging to the specific gender or speaker.\nAccording to the diarization outputs, the clustered and labelled data of the\nanalyzed audio channel were employed for building gender-dependent (GD) or\nspeaker-dependent (SD) AMs needed for gender detection and speaker identi-\nfication. Note that these models are independent from the speech recognition\nOe\nIf annotated acoustic data are available, more precise GD or SD AMs can be\nbuilt using the audio segments automatically extracted according to the manu-\nally generated transcriptions. The data from the diarization may be replaced by\nhe extracted audio segments. If we suppose that the diarization toolkit provides\na reliable clustering with low diarization error rate then the generated speech\n\nS\n\n  \n\nsegments can be appropriately used for building AMs for detection.\n\nThe presented transcription system was primarily adapted to the BN data\nhat were used for the offline diarization process and subsequently for building\nAMs for detection and identification. But generally, there is no limitation of\nusing other type of acoustic data. In that case, the diarization followed by GD\nand SD modeling should be carried out again so that the acoustic conditions of\nhe training data and the audio data to be transcribed will match each other.\n\n   \n\n \n\n3.2. Acoustic Feature Extraction\n\nWe found that the optimal segmentation performance can be achieved, when each\nmodule is optimized for the specific task separately. Therefore, the gender detec-\ntion and speaker identification modules contain different advanced front-ends\ncompared to our previous work [19]. More specifically, the AMs for the speaker\nidentification task were trained on features generated by our own method called\nas class-dependent two-dimensional linear discriminant analysis (CD-2DLDA)\nthat was proposed in [24] for speech recognition.\n\n \n\f"
        },
        "0006": {
            "matches": 5,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0006.html",
            "result": "Aut. Transcript. and Subtitl. of Slovak Multi-genre AV Recordings AT\n\nCD-2DLDA extends the classical 2DLDA to the class-dependent approach in\norder to improve the discriminability between the defined classes. In this setup,\nthe speaker labels are treated as classes that are needed for estimation of the\nleft and right transformation matrices. This method employs two-pass recog-\nnition strategy where the first pass utilizes the state-of-the-art 13-dimensional\nmel-frequency cepstral (MFC) coefficients to obtain the label of the class being\nidentified. Durmg the second pass, the class-dependent 2D transformation is\nperformed and the final speaker identification step is carried out.\n\nThe second extended front-end utilizes the heteroscedastic linear discrim-\nimant analysis (HLDA) features [10] that are optimized to improve the gender\ndetection performance. In this case, only three classes are available (male, female,\nor background). They are intended to estimate the classical LDA matrices that\nare further improved by the heteroscedastic iterative optimization criteria.\n\n \n\n3.3 Gender- and Speaker-Dependent Modeling\n\nThe acoustic representations and the corresponding cluster mformation needed\nfor the acoustic modeling were prepared previously in the diarization task (see\nSect. 3.1). They were further transformed by CD-2DLDA and HLDA, depending\non detection/identification task. All GD and SD models were trained as mul-\ntivariate one-state left-to-right Gaussian mixture models (GMMs) with up to\n1,024 PDFs (probability density functions) using the HTK [27]. The GD and\nSD recognizers employed a simple vocabulary, grammar and word net. composed\nfrom the modeled units (genders and speaker IDs). Basically, the mentioned\ndetectors worked as a simple phone-based recognizers where the phone units\nwere replaced by the gender or speaker IDs. The GD and SD acoustic modeling\nframeworks have a number of interesting features. They allow to:\n\ntrain new GD AMs from scratch, if a transcription of new data has to be\nperformed and new acoustic data are available;\n\nretrain an existing GD AM, if additional acoustic data are collected for the\ngenders, whereas the robustness of AMs may be increased;\n\ntrain a new SD AM, if a new speaker appears in the speaker inventory;\nretrain easily an existing SD model, if additional SD data are collected.\n\n3.4 Speech Segmentation\n\nIn general, it is possible to transcribe a continuous audio stream without any seg.\nmentation, but the computation time and decoding may take a very long time [5].\nAutomatic speech segmentation is usually applied to speed wp the recognition\nprocess and to improve the overall performance by identifymg and handling the\nspecific parts in the recognized speech (speaker change boundaries, gender- or\nspeaker-specific segments, non-speech events, different acoustic conditions, etc.).\n\nThe presented transcription system supports two independent modes of the\nspeech segmentation: single- and multi-channel mode. The single-channel seg-\nmentation module was designed to process any kind of a standard single-channel\n\n \n\f"
        },
        "0007": {
            "matches": 5,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0007.html",
            "result": "48 J. Stas et al.\n\naudio stream (broadcast news, discussion, lecture, etc.). The multi-channel mode\nwas primarily designed for BN transcription, where multiple audio streams are\npresent at the same time, containing different types of audio (studio streams,\nexternal streams from reporters or interviewees, jingles, commercials, etc.) (25).\nIn this case, each channel employs independent processing with different configu-\nration and the single-channel stream is transcribed by an independent recognizer.\n\nIn order to employ gender- or speaker-dependent speech recognition and thus\nimprove the transcription performance, gender detection or speaker identification\nhave to be carried out on the single or multi-channel audio stream. The gender\ndetection can be performed using the default GD AMs (the detection rate will be\nsatisfactory, because the models were trained on a sufficient amount of acoustic\nrepresentations for each gender).\n\nAs was mentioned earlier, the speech processing supports new GD AM train-\ning directly from the recognized audio that does not match exactly the acoustic\nconditions of the default AMs. This operation is meaningful only if there are\navailable enough training examples for both genders. The 5D segmentation is not\nsupported implicitly, if the single-channel waveform contains voices of mmknown\nrs that were not included in the SD training data. However, there is a\npossibility to train a new SD AM, if the recognized audio provides a sufficient\namount of speaker examples that originate from the diarization. In the pre-\nsented system, the multi-channel mode supports speaker identification because\nthe speakers in the BN are known and the SD AMs can be at ease prepared in\n\n   \n\n \n\nthe previous training phase. Note that the inner princi\nand SD task for both single- or multi-channel mode ar\n\nThe proposed speech segmentation is a two-level se\nin Fig. 3. The first level is represented by our VAD [2\n\nles of segmentation, GD\ne basically the same.\n\nuential process described\n3] that is responsible for\n\naccurate speech or silence discrimination. In order to determime the VAD labels,\n\nthe waveform is processed in the time domain by overla\nrectangular window with standard length of 25 ms anc\n\noping blocks extracted by\n10 ms frame step. After\n\nre-arranging the samples of the current block into matrix, segmental time domain\n\nprincipal component analysis (PCA) [7] is applied to\nthat, N eigenvalues are computed for each block, w\nof PCA space. The eigenvalues are used to determin\n\nthe sample matrix. After\nnere N is the dimension\ne the nature of the i-th\n\nsegment (voice/silence)} using a thresholding criterion [23]. In this way, the whole\n\nwaveform is described by VAD coefficients that are fur\naverage window. In our setup, we set N to 4 or 2 for\nor 8kHz, respectively. For detailed description of the\n\nther smoothed by moving\nsignals with f, = 16 kHz\n\ndescribed VAD, see [23].\n\nCompared to our previous work [19], the segmentation module was consid-\n\nerably improved due to the fact that very short spee\n\nthe segmentation output (< 2s). This effect caused p\nsimilar) segments that did not have good influence on\n\nch segments occurred at\nosroducing one-word (and\nthe recognizer.In order to\n\neliminate this phenomenon, we designed a container-like buffer that is applied\n\n \n\nto all generated speech-active segments. They are seq\nthe buffer until the predefined container length is reac\nmulated segments are joined together with respect to\n\n \n\nuentially accumulated in\naed. After that, the accu-\nthe original silence gaps.\n\n \n\n \n\f"
        },
        "0008": {
            "matches": 5,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0008.html",
            "result": "Aut. Transcript. and Subtitl. of Slovak Multi-genre AV Recordings AQ\n\nWe set the length of the container to approx. 20s, thus the lengths of segments\nthat are fed to the second segmentation level lie around the predefined value.\nThe second segmentation level employs the Viterbi algorithm and it is respon-\nsible for precise GD or SD segmentation. In other words, GD and SD recogniz-\ners are run to detect and locate gender- and speaker-change points that split\nthese regions into gender- or speaker-homogeneous segments. At this stage, time\nstamps are generated with gender labels and if needed, they can be extended\nwith speaker labels. The overall segmentation requires final time synchronization\nbetween the first and second level due to eliminated silent parts at the first level.\n\n4 Speech Recognition Server\n\nTo support wide variety of applications the server-based speech recognition was\nadopted. The main advantage of the server-based solution is that the software\nand algorithms used can be optimized for one hardware configuration and under\nsupervision of an expert, while the service can be provided to the users without\nany expert knowledge. This way any kind of devices, especially devices with low\ncomputation resources can be equipped with speech recognition capability. In\nour case the advantage is that this solution separates more complex system from\n\n \n\nspeech recognition making it modular, scalable, easy to maintain and reusable\n\nfor other purposes beyond this paper.\n\n \n\n   \n  \n   \n \n \n \n\nspeech\nrecognition\nserver\n\nrecognition\n\n \n\nspeech segmentation model combination 4\n\ngender identification model combination 2\n\nspeaker diarization\n\nrecognition\n\nrecon se\n\n3 23\ng ae\n& ea\n2\n\n8 3\n3\n\n=\n\ng\n\ns\n\ngs\n\n \n\n   \n\nFig. 4. Multi-threaded speech recognition server\n\n \n\nAccording to the thought the whole system is broken into client and server\npart. Client system is responsible for extracting as much information as possible\nfrom input audio recording including speech segmentation, speaker diarization\nand gender identification while the server is responsible for speech recognition\n\ns ig. 4). The main idea is to use as much information as possible to identify\ntic and language model combination for subsequent speech recognition.\nLogically can be assumed that more matching AM to the speaker and channel\ncharacteristics and LM to the topic can increase the speech recognition accuracy.\nAnother approach is to use multiple combination of AMs and LMs and combine\noutput hypotheses to further increase the accuracy [4]. In our system we have\nused both approaches represented by a rule based identification for the selection\nof multiple best model combinations.\n\n \n\n     \n\n \n\n \n\f"
        },
        "0010": {
            "matches": 9,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0010.html",
            "result": "Aut. Transcript. and Subtitl. of Slovak Multi-genre AV Recordings 51\n\nrecognition server with desired combination of the models. To select the right\ncombination of models a set of rules are used depending on completeness of infor-\nmation extracted. If the gender identification is available then gender-dependent\nmodel combination is used along with gender-independent and their outputs\nare later combined. If no gender identification is available then only gender-\nindependent model combination is used. In case of selecting more than one model\ncombinations, the outputs are merged to single hypothesis using the ROVER\nalgorithm [4]. In fact a modified version of the ROVER is used with additional\npreprocessing of input hypothesis that consists from score smoothing [12].\n\nThe ROVER algorithms is following. First it incrementally creates word tran-\nsition network from input hypothesis while it aligns only two of them at a time.\nSubstitutions and deletions are handled using NULL transition score C(@) that\ncan be estimated on a development set. In the next step it uses voting mecha-\nnism based on confidence score for each alternative word C(w,) and its occur-\nrence N(w,) normalized by count of the combined systems according Formula 1\nto select the resulting word alternative.\n\n \n\n \n\n+ (1 = aC (uy). (1)\n\n \n\nConfidence score C(w,) for each word alternative can be computed as average\nor maximum score found in input hypotheses [12]. In our case, we empirically\nestimated the C(@) score to 0.5 and a to 0.7 in order to control the balance\nbetween the number of word occurrences and its confidence scores. The men-\ntioned smoothing of input confidence scores is done according Formula 2.\n\n \n\nClay) = 6C (uy) + A ~ BC (uy). Q)\n\nThe new score C(u,) for word w; is computed from previous word in time\nw,.., and 3 is smoothing factor controlling amount of transfered score.\n\n4.3 Acoustic Modeling for Speech Recognition\n\nIn presented system we have used gender-dependent and gender-independent\nAMs. Both AMs are based on triphone context-dependent 3-state hidden Markov\nmodels (HMMs) with 32 probability density functions (PDFs) on a state and on\na feature vector type MFC coefficients with zeros, delta and acceleration coeffi,\ncients and cepstral mean normalization removed (MFCC_0DAZ). The training\nprocess was modified, tree-based state tying was replaced by the effective tri-\nphone mapping algorithm [18]. The audio format of recordings for training were\n16kHz 16 bit PCM mono. The gender-independent AM was trained on manu-\nally annotated speech recordings from the database of judicial readings (100h);\nread phonetically rich sentences and newspaper articles (150h); broadcast news\n(250h); parliament speech (90h); and the Court TV shows (80h) [15,18,25].\nGender-dependent AMs were created by splitting the training database for each\ngender and building two separate AMs for females and males.\n\n \n  \n\n \n\n \n\n \n\f"
        },
        "0012": {
            "matches": 2,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0012.html",
            "result": "Aut. Transcript. and Subtitl. of Slovak Multi-genre AV Recordings 53\n\nWord error rate (WER) was used to evaluate overall performance of the\nproposed automatic transcription system on the test data. WER is computed\nby comparing reference annotations against recognized result as follows:\n\n   \n\nrp + Noew + s\n\nWER= x 100 [%], (4)\n\nNrer\nwhere Neup refers to the number of substituted words, Nog, is related to\nwords, which are missed out, Nyxs indicates the number of words incorrectly\nadded by the recognizer, and Nexr is the total number of words in the reference.\n\nIn order to evaluate the performance of the proposed system, each recording\nhas been recognized in configuration setup with both background acoustic and\nlanguage models and models adapted to the specific topic, gender and speaking\nstyle of the speaker. The output hypotheses were combined using a modified\nROVER algorithm and evaluated for each speaker separately. After that, labelled\nspeech transcriptions has been created and compared with the reference.\n\nTable 1 summarizes the average state-of-the-art out-ofvocabulary (OOV)\nword rates [%], language model perplexity (PPL) and word error rates\n(WER) [%] evaluated separately for each speaker in the context of Slovak multi-\ngenre automatic speech recognition and transcription.\n\nAs we can see from the Table 1, the speech recognition results are promising\nand comparable with other European languages considering the fact. that the Slo-\nvak language is a highly inflectional and more complex than the other European\nlanguages. The overall performance of the proposed automatic transcription and\nsubtitling system was increased relatively by about 8.27% WER in the context of\nbroadcast news transcription task, about 9.62% WER in transcription of Slovak\nTEDx talks and about 25.98% for parliament speech.\n\nIt can be concluded that using multiple speech recognizers in various of con-\nfigurations and application of gender-dependent acoustic modeling, adaptation\nof language models to the specific topic and speaking style, and utilization of\nN-best rescoring on the word recognition level we achieved further improvement\nin transcription of semi-spontaneous and spontaneous Slovak speech.\n\n \n\n \n\n6 Discussion and Future Work\n\nAutomatic transcription works well without any known bugs, but system is still\nunder the development. Several new features are gradually added to the current\nprototype. Although the initial experimental evaluation reports that the word\nerror rates are promising, there is still room for some new innovations, system\ntuning and improvements of the underlying speech technology. There is a nwm-\nber of challenging tasks associated with the further development of the current\nspeech transcription system leading to more promising performance. Moreover,\nthe system's modularity, its interesting features and modern approaches makes\nit highly applicable in many other domain-oriented tasks.\n\nIf we look at the server-based speech recognition, there are several possibili-\nties of improvements. One of the most challenging tasks is to apply the discrim-\ninative feature transformations in the block of feature extraction and acoustic\n\n \n\n \n\f"
        },
        "0013": {
            "matches": 1,
            "pathToPage": "./_data/s/st/stasAutomaticTranscriptionSubtitling2018/pages//0013.html",
            "result": "54\n\nTable 1. Overall spe\n\nJ. Stas et al.\n\n \n\nbroadcast news and TEDx talks\n\nSpeaker\n\njoal iba\nleit Lhe\nmgil tha\nrem tba\nwtol tba.\nthat the\nphr tia\nphat tha\nppal tha\n\nveut tba\n\nbidet Sha\n\n \n\nsmil Sha\ndoctSba\nmibal ibe.\nniko Tika\nmsil Sha\n\nmsp1fba\n\n \n\n1D Gender Segments Re;\n\n \n\n> words OOV [%\n\n \n\nParliament speech\n\n   \n   \n   \n   \n   \n   \n   \n   \n   \n  \n \n   \n   \n    \n   \n   \n   \n   \n   \n   \n  \n    \n  \n    \n \n \n \n\n428.70 14.56\nB9L.22 14.17\n73L49 7.89\n13.73\n11.89\n\n1319\n(\"240\n684\n386\n1733\n1130\n4304\n931\n1924\n\n7.62\n426.83 18.15\n611.83 11.61\n179.69 14.81\n\n \n\n2309\n\n1020\n374\n\n9.61\n7.22\n10.88\n20.75\n92\n20.01\n\n1219\n1036\n2764\n2684\nB54\n631\n848\n22.38\nTEDs talks\n\n \n\ndfat Sha F 180\nyma ke F (129\nmel 2he. F 112 1196\nmani dhe F 383 2433\neunt fhe F 168 1627 22.19\naku tba M 229 1738 17.09\nduvet Ske M 307 3198 | 22\nribald gin M 136 1487\n“ysel 2ba M 99 ' S71 31.80\ntrot Ske M 377 3721 4.27 390.29 32.57\nmodeling.\n\nBackgr. models\nPPL WER [%] PPL\n\n16.02\n\n \n\nAdapt. models\n\n227.40 10.99\n24 8.33\n2.54 4.24\n8.\n6.29\n\n \n\n \n\n   \n\n5\n\n6.02\n17.40\n10.88\n11.82\n\n8.14\n6.42\n\n472.76\n165.28\n587.38\n341.29\n38.\n230.68\n\n \n\n432.22 31.80\n\n \n\n  \n\n28. 45.59\nBTL.G5 42.47\n389.30 42.75\n\n \n\n523.96 18.68\n\n \n\n298.09 31.79\n\n \n\nh recognition results evaluated on speech recordings obtained\nfrom 10 native Slovak speakers for each of the 3 different domains: parliament spe\n\nh,\n\n[A Relative\n\nWER [%) | ¥\n\n9 12.04\n\n \n\n \n\nFR [%}\n\n21.00\n4.13\n\n \n\n29\n20.19\n\n   \n\n \n\n15.03\n15.82\n9.07\n\n \n\nRegarding to the automatic speech segmentation, we are planning\n\nto find an optimal configuration of features for gender- and speaker-dependent\nmodeling. Also, more sophisticated decoding based on finite state transducers\n\nsuch as\n\nAcknowledgement. The\n\nKaldi for speech recognition [16] is necessary\n\n \n\n \n\nresearch in this paper was supported by the Faculty of\n\nElectrical Engineering and Informatics at the Technical University of KoSice under the\n\nresi\n\n \n\nsarch project FEI-2015-30, by the Ministry of Education, Sc\nSport of the Slovak Republic under the research proje\nSlovak Res\n\narch and Development Agency under the re:\n\n \n\n   \n\n \n\nence, Research and\n4 VEGA 1/0511/17 and the\narch project APVV-15-0517.\n\f"
        }
    },
    "wiriyathammabhumComputerVisionNatural2016": {
        "0008": {
            "matches": 1,
            "pathToPage": "./_data/w/wi/wiriyathammabhumComputerVisionNatural2016/pages//0008.html",
            "result": "71:8 P Wirlyathammabhum et al.\n\nTable Hl. Bloom’s Taxonomy Applied to Vision and Language. The Framework Amounts to\nDeveloping Systems That Can Answer Questions About a Scene (image/Video). These\nQuestions Form a Hierarchy Starting from Simpler Ones Whose Answer Can Be Easily\nDiscovered in the Scene Itself to Progressively More Difficult Questions Whose Answer\n\nMust Be Inferred, Planned, Visualized, or Reasoned. Many of the Harder Questions\nHave to Do With intention, Causation, Prediction, and Utility Optimization\n\n \n\nBloom’s Taxonomy Vision and Language Understanding through asking ques-\ntions about the scene\n(images/videos)\n\n \n\n \n\nWhat and where questions. The system should recognize nouns,\nKNOWLEDGE attributes, verbs, action and label events\n\n(have seen this before)\n\nWhy and whats next questions. The system should recognize inten-\ntion, causality and should be able to predict the immediate future\nCOMPREHENSION (lean predict what will happen).\n\nHew te questions. The system should be able te perform a command\nAPPLICATION given in language\n\n(like stir the coffee)\n\nHew much, how many, how similar, how different questions. The\nANALYSIS system should be able to recognize and relate the constituent com-\nponents of an event.\n\nHew to achieve a goal questions. The system should be able to pro-\n\n \n\n \n\n \n\n \n\n \n\nSYNTHESIS duce plans by using the event space.\nHew good, how fast, how accurate questions. The system should be\nEVALUATION able te optimize in the space of events\n\n \n\n(or plans)\n\n \n\nthat they are related. An image in the news [Berg et al. 2004], for example, is likely\nto have a face, and the accompanying news text will help identify the face, including\na lot of side information about that person such as occupation, age or gender. This\nrequires that the coreference between the news text and the face can be accurately\ndiscovered. Another example of finding the relationship between images and texts is\nentry-level categorization [Ordonez et al. 2013] by finding its natural description, the\nway people refer to an object in practice. A word makes sense when used to describe a\nspecific object depending on contexts. That word can be easily used for an appropriate\nand unambiguous communication if it is the word people use naturally.\n\nLanguage and visual data provide two sets of information that are combined into the\nwhole story. This conforms to the theory of semiotics [Greenlee 1978], which is the study\nof the relations of signs and their meanings. Semiotics has an analogous viewpoint to\nthe NLP concepts from the previous section [Morris 1938]. First, semiotics studies the\nrelationship between signs and meaning—the semantics of signs. Second, the formal\nrelation between signs is equivalent to syntax. Third, the way humans interpret signs\nin context is equivalent to pragmatics. If we consider purely visual signs, then this leads\nto the conclusion that semiotics can also be approached by computer vision, extracting\ninteresting signs for NLP to realize the corresponding meanings.\n\nThe tasks for language and vision in multimedia mainly fall into three categories:\nvisual properties description, visual description, and visual retrieval.\n\n \n\n \n\n2.1. Visual Properties Description\n\n2.1.1. Altribute-Based Vision. Associating words and pictures [Barnard and Forsyth\n2001] is a form of the recognition task in CV. Object recognition traditionally tries\nto categorize an image to a fixed set of name tags. Farhadi [2011] argues that an image\n\nACM Computing Surveys, Vol. 49, No. 4, Article 71, Publication date: December 2016.\n\f"
        }
    }
}